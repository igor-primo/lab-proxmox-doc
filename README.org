#+TITLE: Roteiro Kubernetes

* Introdução
Essa documentação utiliza princípios de programação
letrada. Consequentemente, alguns snippets que são descritos abaixo então
contidos em scripts extraídos da documentação. O nome do script
que contém o snippet respectivo está em um comentário na primeira
linha do snippet. 

Todos os scripts e comandos devem
ser executados como usuário raiz.

O login de usuário root via SSH deve
estar habilitado também.
* Clonar VMs
A clonagem das VMs é feita através do ProxMox manualmente.
* SSH
Criação de chaves assimétricas para acesso SSH sem senha. Esse passo deve ser
feito manualmente.
** Cria chaves assimétricas
#+begin_src sh
  # A variável HOSTS deve conter os IPs dos hosts da instalação do cluster.
  # Exemplo: HOSTS=(10.20.1.123 10.20.1.124)
  HOSTS=()
  # Gera uma chave RSA
  ssh-keygen -q -t rsa -b 4096 -f ~/.ssh/id_rsa -P ""
  # Copia a chave pública correspondente para os hospedeiros remotos
  for HOST in "${HOSTS[@]}"; do ssh-copy-id $USER@$HOST; done
#+end_src
* Instalar Python
Instalação de Python e Pip no host de onde partirá a instalação
do kubernetes. O seguinte snippet instala Python e Pip em sistemas
baseados em RPM. Caso o sistema ponto de partida não seja baseado em
RPM, deve-se utilizar os comandos cabíveis.
#+begin_src sh :tangle deps-install.sh
  # ./deps-install.sh
  sudo dnf install python39 python3-pip -y
  pip3 install --upgrade pip
#+end_src
* Clonar repositório do Kubespray e instalar dependências.
Caso a instalação esteja partindo de uma máquina exterior ao cluster,
pode-se utilizar o seguinte script; mas se a instalação estiver
partindo de um dos nós do cluster, deve-se omitir a instalação do do
ambiente virtual Python.

O script abaixo instala as dependências Python do Kubespray no
diretório /tmp/repos/kubespray-venv. Segundo a [[https://docs.python.org/3/library/venv.html][documentação]] do Python,
esses ambientes virtuais servem para resolver problemas de
incompatibilidade entre versões de um mesmo software requerido em
várias versões. Em alguns dos scripts/snippets que seguem,
utilizaremos essas dependências por conveniência.
#+begin_src sh :tangle kubespray-install.sh
  # ./kubespray-install.sh
  if [ ! -d ./kubespray-2.22.0 ]; then
	  wget https://github.com/kubernetes-sigs/kubespray/archive/refs/tags/v2.22.0.tar.gz
	  tar -xf v2.22.0.tar.gz
	  rm v2.22.0.tar.gz
  fi

  EXTERNAL=""
  read -p "A instalação do cluster parte de uma máquina externa ao cluster? (y/N)" EXTERNAL
  if [ $EXTERNAL = "y" ]; then
	pip install virtualenv

	VENVDIR=kubespray-venv
	KUBESPRAYDIR=kubespray-2.22.0
	ANSIBLE_VERSION=2.12
	virtualenv  --python=$(which python3) $VENVDIR
	source $VENVDIR/bin/activate
	cd $KUBESPRAYDIR
	pip install -U -r requirements-$ANSIBLE_VERSION.txt
  elif [ $EXTERNAL = "N" ]; then 
	cd kubespray-2.22.0
	pip install -U -r requirements.txt
  else
	echo "Entrada inválida"
  fi
  #+end_src
* Atualizar sistemas
Caso não tenha sido feito ou precisa fazer novamente, atualizar os
sistemas operacionais. Esse script depende da existência dos arquivos
"inventory.ini" e "update-systems-playbook.yaml" que são providos
nesse repositório.

Esse é playbook Ansible que especifica a tarefa para atualizar os pacotes do
sistema operacional alvo para as versões mais recentes da
distribuição. Não precisa ser modificado.
#+begin_src yml :tangle update-systems-playbook.yaml
- name: Atualiza sistema
  hosts: server*
  become: true
  tasks:
  - name: Atualiza sistema
    package:
      name: '*'
      state: latest
#+end_src
Esse é o inventário contendo configuração para acesso aos hosts. Esse arquivo
deve ser atualizado com os IPs dos hosts do cluster manualmente.
#+begin_src text :tangle inventory.ini
server1 ansible_host=10.20.1.113
server2 ansible_host=10.20.1.115
#+end_src
Esse é o mini-roteiro a ser utilizado para aplicar o YAML anterior.
#+begin_src sh :tangle update-systems.sh
  # ./update-systems.sh
  (
	  # Se o diretório ./kubespray-venv existe, assuma que
	  # a instalação parte de uma máquina exterior ao cluster
	  # e carregue as variáveis do ambiente virtual
	  if [ -d ./kubespray-venv ]; then
		  source ./kubespray-venv/bin/activate
	  fi
	  ansible-playbook -i inventory.ini update-systems-playbook.yaml --become --become-user=root
  )
#+end_src
* Desabilitar firewall
Caso os firewalls não tenham sido desabilitados, desabilitá-los.

Esse é o playbook Ansible utilizado para instruir o Ansible para
executar os comandos para parar e desabilitar o serviço de firewall
nas máquinas alvo.
#+begin_src yml :tangle remove-firewall-playbook.yaml
- name: Remove firewall e habilitar login de root via SSH
  hosts: server*
  become: true
  tasks:
  - name: Remove firewall
    shell: |
      systemctl stop firewalld.service
      systemctl disable firewalld.service
#+end_src
Esse é o snippet para aplicar a configuração.
#+begin_src sh :tangle remove-firewall.sh
  # ./remove-firewall.sh
  (
	  # Se o diretório ./kubespray-venv existe, assuma que
	  # a instalação parte de uma máquina exterior ao cluster
	  # e carregue as variáveis do ambiente virtual
	  if [ -d ./kubespray-venv ]; then
		  source ./kubespray-venv/bin/activate
	  fi
	  ansible-playbook -i inventory.ini remove-firewall-playbook.yaml --become --become-user=root
  )
#+end_src
* Próximos passos
Próximos passos são documentados no repositório oficial do [[https://github.com/kubernetes-sigs/kubespray][Kubespray]].
Mas há algumas ressalvas. A instalação utilizando ambientes virtuais
do Python quando a instalação é feita a partir de uma máquina que será
um nó do cluster apresenta erros na busca de dependências do python.
Mas como caso a máquina deva ser parte do cluster esse roteiro instrui
para não utilizar ambientes virtuais do Python para instalar as
dependências do Kubespray, esse problema já foi contornado.

Os seguintes passos, que são os mais importantes, devem ser executados manualmente:
#+begin_src sh
  # Na raíz do projeto kubespray.
  cd kubespray-2.22.0
  
  cp -rfp inventory/sample inventory/mycluster
  # IPS é um vetor contendo os IPs dos hosts do cluster.
  declare -a IPS=()

  # A seguinte linha deve ser executada somente se a máquina de onde parte
  # a instalação for externa ao cluster.
  source ../kubespray-venv/bin/activate

  # Esse script gera o inventário automaticamente com configuração padrão.
  CONFIG_FILE=inventory/mycluster/hosts.yaml python3 contrib/inventory_builder/inventory.py ${IPS[@]}
  # Nesse ponto pode-se revisar e modificar as variáveis em
  # inventory/mycluster/group_vars/all/all.yml e
  # inventory/mycluster/group_vars/k8s_cluster/k8s_cluster.yml.

  # Para limpar um cluster velho, executar como root:
  ansible-playbook -i inventory/mycluster/hosts.yaml --become --become-user=root reset.yml
  # Para fazer uma nova instalação do kubernetes, executar como root:
  ansible-playbook -i inventory/mycluster/hosts.yaml --become --become-user=root cluster.yml
#+end_src
* Instalação do Wordpress + MySQL
Os passos utilizados para instalação do Wordpress e MySQL consistem
na aplicação de um [[https://kubernetes.io/docs/concepts/workloads/controllers/deployment/][deployment]] e de um [[https://kubernetes.io/docs/concepts/services-networking/service/][service]] para cada
componente. Cada deployment possuirá também uma configuração para
utilizar um servidor NFS como armazenamento persistente, com o intuito
de preservar a configuração das aplicações e dos dados do banco de dados entre
possíveis deployments (ex.: um nó é desligo ou cai).
Também possuirá uma configuração que especifica uma [[https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/][toleration]], i.e.,
um espaço de tempo que um container permanecerá atrelado a um nó
enquanto uma taint for verificada, por exemplo, quando a taint
not-ready estiver verificada quando o nó estiver fora do ar. Para mais
detalhes sobre o que esses termos significam, vide a documentação
referenciada nos links nesse parágrafo.
** Instalação do servidor NFS
Antes de tudo, executamos o seguinte roteiro para criar um servidor
NFS. No host onde o servidor NFS será hospedado, executar, como raiz:
#+begin_src sh
  # Como raiz.
  dnf install nfs-utils -y
  mkdir /var/nfs/general -p
  touch /etc/exports

  # Colocar IPs dos workers no vetor HOSTS.
  # Exemplo: HOSTS=(10.20.1.113 10.20.1.118)
  # Exportamos a variável para podermos utilizar essa variável em outros momentos, caso cabível.
  export HOSTS=()
  
  for i in "${HOSTS[@]}"; do echo "/var/nfs/general $i(rw,no_subtree_check,no_root_squash)" >> /etc/exports;done

  systemctl enable nfs-server
  systemctl start nfs-server

  # Esse comando deve ser executado toda vez que o arquivo /etc/exports
  # for modificado.
  exportfs -ra

  # Os hosts clientes também precisam do pacote nfs-utils, caso não estejam instalados
  # então instalamos ele:
  for i in "${HOSTS[@]}"; do ssh $USER@$i "dnf install nfs-utils -y";done
#+end_src
Caso o servidor NFS já exista, deve-se executar apenas os seguintes comandos no servidor:
#+begin_src sh
  # Modificar manualemente o arquivo /etc/exports
  # ou então executar o seguinte snippet.
  HOSTS=()
  for i in "${HOSTS[@]}"; do echo "/var/nfs/general $i(rw,no_subtree_check,no_root_squash)" >> /etc/exports;done

  exportfs -ra
#+end_src
Os comandos acima especificam o diretório a ser montado nos clientes,
os IPS dos clientes e configurações por IP.

Deve-se também criar pastas específicas de cada aplicação no diretório
/var/nfs/general/
(ex.: /var/nfs/general/mysql-igor)
e deixá-las com permissão 777 para evitar erros de permissão e também
com usuário e grupo nobody. Os comandos são esses, por exemplo:
#+begin_src sh
  chmod -R 777 /var/nfs/general/mysql-igor
  chown -R nobody:nobody /var/nfs/general/mysql-igor
#+end_src
** Cópia dos arquivos de configuração para o cluster
Copie os arquivos de configuração mysql-dep.yml, mysql-serv.yml,
wordpress-dep.yml e wordpress-serv.yml para um master do cluster utilizando o
comando, na raíz do projeto:
#+begin_src sh
  scp wordpress/*.yml root@<HOST-IP>
#+end_src
Onde HOST-IP é o IP de um dos control_planes do cluster.
** Troubleshooting
Para fazer troubleshooting, visualizar logs e informações sobre as
ações do kubernetes pode-se utilizar esses comandos:
#+begin_src sh
  # Lista deployments
  kubectl get deployments -o wide

  # Lista pods
  kubectl get pods -o wide

  # Lista serviços
  kubectl get svc -o wide

  # Visualiza detalhes sobre um recurso ou grupo de recursos específico
  kubectl describe deployments
  kubectl describe deployment <DEPLOYMENT_NAME>

  # Visualiza logs emitidos por um pod
  kubectl logs --follow <POD_NAME>

  # Para ver os detalhes de todos os comandos possívels
  kubectl --help | less
#+end_src
** Aplicação do Deployment do MySQL
Logado em um dos master nodes (control_planes) modificar o seguinte
arquivo de configuração para servir suas necessidades, como o caminho
para o diretório dos arquivos da aplicação no servidor NFS.
#+begin_src txt :tangle wordpress/mysql-dep.yml
# Arquivo: mysql-dep.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mysql-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql:latest
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: password
        ports:
        - containerPort: 3306
        volumeMounts:
        - name: nfs-volume
          mountPath: /var/lib/mysql
      volumes:
      - name: nfs-volume
        nfs:
          server: 10.20.1.111
          path: /var/nfs/general/mysql-igor
          readOnly: no
      tolerations:
      - effect: NoExecute
        key: node.kubernetes.io/not-ready
        operator: Exists
        tolerationSeconds: 30
      - effect: NoExecute
        key: node.kubernetes.io/unreachable
        operator: Exists
        tolerationSeconds: 30
#+end_src
Depois execute o seguinte comando para levantar o deployment do MySQL.
#+begin_src sh
  kubectl apply -f mysql-dep.yml
#+end_src
** Aplicação do Service do MySQL
O seguinte arquivo configura o serviço para o MySQL. Caso queira, pode
modificar a porta de acesso externo serviço do pod modificando o campo
"targetPort".
#+begin_src txt :tangle wordpress/mysql-serv.yml
# Arquivo: mysql-serv.yml
apiVersion: v1
kind: Service
metadata:
  name: mysql-service
spec:
  selector:
    app: mysql
  ports:
    - protocol: TCP
      port: 3306
      targetPort: 3306
#+end_src
Utilize o seguinte comando para aplicar a configuração do serviço MySQL.
#+begin_src sh
  kubectl apply -f mysql-serv.yml
#+end_src
** Aplicação do Deployment do Wordpress
Novamente, revise o seguinte arquivo de configuração do deployment
para o Wordpress e modifique os campos que forem necessários, como o
para os arquivos específicos do Wordpress no servidor NFS.
#+begin_src txt :tangle wordpress/wordpress-dep.yml
# Arquivo: wordpress-dep.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: wordpress-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: wordpress
  template:
    metadata:
      labels:
        app: wordpress
    spec:
      containers:
      - name: wordpress
        image: wordpress:latest
        env:
        - name: WORDPRESS_DB_HOST
          value: mysql-service
        - name: WORDPRESS_DB_USER
          value: root
        - name: WORDPRESS_DB_PASSWORD
          value: password
        - name: WORDPRESS_DB_NAME
          value: wordpress
        ports:
        - containerPort: 80
        volumeMounts:
        - name: nfs-volume
          mountPath: /var/www/html
      volumes:
      - name: nfs-volume
        nfs:
          server: 10.20.1.111
          path: /var/nfs/general/wordpress-igor
          readOnly: no
      tolerations:
      - effect: NoExecute
        key: node.kubernetes.io/not-ready
        operator: Exists
        tolerationSeconds: 30
      - effect: NoExecute
        key: node.kubernetes.io/unreachable
        operator: Exists
        tolerationSeconds: 30
#+end_src
Utilize o seguinte comando para aplicar o deployment do Wordpress.
#+begin_src sh
  kubectl apply -f wordpress-dep.yml
#+end_src
** Aplicação do Service do Wordpress
Revise o arquivo de configuração do serviço Wordpress e modifique os
campos que achar necessário.
#+begin_src txt :tangle wordpress/wordpress-serv.yml
# Arquivo: wordpress-serv.yml
apiVersion: v1
kind: Service
metadata:
  name: wordpress-service
spec:
  selector:
    app: wordpress
  type: NodePort
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
      nodePort: 30036 
#+end_src
O seguinte comando aplica a configuração do serviço Wordpress.
#+begin_src sh
  kubectl apply -f wordpress-serv.yml
#+end_src
** Criação do banco de dados MySQL no container
Para que o Wordpress funcione precisamos criar, manualmente, um banco
de dados chamado 'wordpress'. Para isso, logamos no container do MySQL
e utilizamos o utilitário 'mysql' para emitir o comando de criação do
banco de dados.

A partir de control_plane logar no container:
#+begin_src sh
  kubectl get pods
  kubectl exec -it <MYSQL_POD_NAME> -- bash
  mysql -u root -p
  # no prompt do shell do mysql:
  create database wordpress;
  exit
  exit
#+end_src
* Instalação do Gitlab
** Aplicação do Deployment para o Gitlab
Revise o arquivo de configuração do deployment para o Gitlab e edite
os campos necessários, como os caminhos nos volumes "gitlab-data",
"gitlab-logs" e "gitlab-config" para servir a sua configuração. Vale
ressaltar que os caminhos para esses volumes devem ser diferentes.
#+begin_src txt :tangle gitlab/gitlab-dep.yml
# Arquivo: gitlab-dep.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gitlab-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: gitlab
  template:
    metadata:
      labels:
        app: gitlab
    spec:
      containers:
      - name: gitlab
        image: gitlab/gitlab-ce:latest
        env:
        - name: GITLAB_OMNIBUS_CONFIG
          value: |
            external_url 'http://localhost'
        ports:
        - containerPort: 80
        volumeMounts:
        - name: gitlab-data
          mountPath: /var/opt/gitlab
        - name: gitlab-logs
          mountPath: /var/log/gitlab
        - name: gitlab-config
          mountPath: /etc/gitlab
      volumes:
      - name: gitlab-data
        nfs:
          server: 10.20.9.111
          path: /var/nfs/general/gitlab-igor/data
          readOnly: no
      - name: gitlab-logs
        nfs:
          server: 10.20.9.111
          path: /var/nfs/general/gitlab-igor/logs
          readOnly: no
      - name: gitlab-config
        nfs:
          server: 10.20.9.111
          path: /var/nfs/general/gitlab-igor/config
          readOnly: no
      tolerations:
      - effect: NoExecute
        key: node.kubernetes.io/not-ready
        operator: Exists
        tolerationSeconds: 30
      - effect: NoExecute
        key: node.kubernetes.io/unreachable
        operator: Exists
        tolerationSeconds: 30
#+end_src
Execute o seguinte comando para aplicar a configuração do deployment:
#+begin_src sh
  kubectl apply -f gitlab-dep.yml
#+end_src
** Aplicação do Service para o Gitlab
Revise o arquivo de configuração para o serviço do Gitlab e edite o
que achar necessário.
#+begin_src txt :tangle gitlab/gitlab-serv.yml
# Arquivo: gitlab-serv
apiVersion: v1
kind: Service
metadata:
  name: gitlab-service
spec:
  selector:
    app: gitlab
  type: NodePort
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
      nodePort: 30036 
#+end_src
Execute o seguinte comando para aplicar a configuração:
#+begin_src sh
  kubectl apply -f gitlab-serv.yml
#+end_src
* Instalação de um Runner no Gitlab
Antes de de fato instalarmos o Runner do Gitlab, precisamos
registrá-lo no Gitlab. Para isso, instalamos o Runner num container Docker
apenas para registrá-lo no Gitlab. Depois descartamos o container e
instalamos outro Runner utilizando kubectl e utilizamos a configuração
do outro Runner, modificada, para configurar o novo Runner.
** Instalação do Docker
#+begin_src sh :tangle ./install-docker.sh
  # ./install-docker.sh
  dnf config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
  dnf upgrade --refresh -y
  dnf install docker-ce docker-ce-cli containerd.io
  systemctl start docker
#+end_src
** Logar numa instância Docker do Gitlab Runner
#+begin_src
docker run -it --entrypoint /bin/bash gitlab/gitlab-runner:latest
#+end_src
** Registrar o Gitlab Runner
*** Gerar um token para registrar o Gitlab Runner
Crie um repositório teste na sua instância do Gitlab. Acesse o
repositório teste e na tela do repositório vá em Settings -> CI/CD ->
Runners e siga as instruções para registrar um novo runner. O runner
no nosso caso deve utilizar a plataforma Linux e deve ser configurado
para executar trabalhados sem tag acionando o checkbox "Run untagged
jobs".

Você será direcionado para uma tela onde constará o token gerado.
*** Registrar
Volte para o shell logado no container Docker e execute:
#+begin_src sh
gitlab-runner register --url <CAMINHO_PARA_O_CLUSTER> --token <TOKEN_GERADO>
#+end_src
O CAMINHO_PARA_O_CLUSTER é o IP para qualquer nó do cluster (ex.:
http://10.20.9.116:30036).
Você pode visualizar o arquivo de configuração, gerado
automaticamente, do Runner com o comando:
#+begin_src sh
more /etc/gitlab-runner/config.toml
#+end_src
Nós vamos utilizar esse arquivo para configurar o nosso container
Kubernetes do Gitlab Runner. No meu caso o arquivo é estruturado
assim:
#+begin_src txt :tangle ./gitlab/config.toml
concurrent = 1
check_interval = 0
shutdown_timeout = 0

[session_server]
  session_timeout = 1800

[[runners]]
  name = "runner"
  url = "http://10.20.9.116:30036"
  id = 1
  token = "glrt-qJDS_pTGimZC8YtaoBPw"
  token_obtained_at = 2023-05-31T16:40:36Z
  token_expires_at = 0001-01-01T00:00:00Z
  executor = "docker"
  [runners.cache]
    MaxUploadedArchiveSize = 0
  [runners.docker]
    tls_verify = false
    image = "busybox:latest"
    privileged = false
    disable_entrypoint_overwrite = false
    oom_kill_disable = false
    disable_cache = false
    volumes = ["/cache"]
    shm_size =0 
#+end_src
Agora podemos deslogar do container Docker e derrubar o serviço Docker
e proceder para os próximos passos. Podemos também parar o serviço Docker:
#+begin_src sh
systemctl stop docker
#+end_src
*** Configurar o Gitlab Runner do Kubernetes
Para configurar o Gitlab Runner, precisamos adicionar um
ServiceAccount, um Role e um RoleBinding respectivo ao Runner no
cluster. O seguinte
arquivo provê essa configuração:
#+begin_src txt :tangle ./gitlab/gitlab-runner-authentication.yml
# Arquivo: gitlab-runner-authentication.yml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: gitlab-admin
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: gitlab-admin
rules:
  - apiGroups: [""]
    resources: ["*"]
    verbs: ["*"]

---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: gitlab-admin
subjects:
  - kind: ServiceAccount
    name: gitlab-admin
roleRef:
  kind: Role
  name: gitlab-admin
  apiGroup: rbac.authorization.k8s.io
#+end_src
Aplique essa configuração com o comando:
#+begin_src sh
kubectl apply -f gitlab-runner-authentication.yml
#+end_src
Depois de termos um ServiceAccount, Role e RoleBinding configurados
precisamos de um ConfigMap para persistir a configuração do runner.
O arquivo de configuração do ConfigMap deve especificar o arquivo de
configuração do runner que roubamos do container Docker com algumas
modificações para adaptá-lo ao ambiente Kubernetes. No meu caso,
ficou assim:
#+begin_src txt :tangle ./gitlab/gitlab-runner-config.yml
# Arquivo: gitlab-runner-config.yml
apiVersion: v1
kind: ConfigMap
metadata:
  name: gitlab-runner-config
data:
  config.toml: |-
     concurrent = 4
     [[runners]]
       name = "runner"
       url = "http://10.20.9.116:30036"
       id = 1
       token = "glrt-qJDS_pTGimZC8YtaoBPw"
       token_obtained_at = 2023-05-31T16:40:36Z
       token_expires_at = 0001-01-01T00:00:00Z
       executor = "kubernetes"
       [runners.kubernetes]
          poll_timeout = 600
          cpu_request = "1"
          service_cpu_request = "200m"
#+end_src
Após a aplicação dessa configuração, aplicamos a configuração do
Deployment:
#+begin_src txt :tangle ./gitlab/gitlab-runner-deployment.yml
# Arquivo: gitlab-runner-deployment.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gitlab-runner
spec:
  replicas: 1
  selector:
    matchLabels:
      name: gitlab-runner
  template:
    metadata:
      labels:
        name: gitlab-runner
    spec:
      serviceAccountName: gitlab-admin
      containers:
        - args:
          - run
          image: gitlab/gitlab-runner:latest
          imagePullPolicy: Always
          name: gitlab-runner
          resources:
            requests:
              cpu: "100m"
            limits:
              cpu: "100m"
          volumeMounts:
            - name: config
              mountPath: /etc/gitlab-runner/config.toml
              readOnly: true
              subPath: config.toml
      volumes:
        - name: config
          configMap:
            name: gitlab-runner-config
      restartPolicy: Always
#+end_src
*** Verificar a instalação do Runner
Você pode verificar se a instalação do Runner obteve sucesso indo para
a tela Settings -> CI/CD -> Runners e checando se há um runner verde
na seção "Assigned project runners".
*** Referência
Vide [[https://adambcomer.com/blog/install-gitlab-runner-kubernetes/][referência]].
** Criação de pipeline CI/CD no Gitlab
** Configuração de Grafana
*** Instalação de Prometheus
**** Deploy
#+begin_src sh
  kubectl create namespace darwin
  helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
  helm install prometheus prometheus-community/kube-prometheus-stack --namespace darwin
#+end_src
#+begin_src txt :tangle ./prometheus/prometheus-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: darwin
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      scrape_timeout: 10s
      evaluation_interval: 15s
    scrape_configs:
      - job_name: 'darwin-service'
        scrape_interval: 5s
        static_configs:
          - targets: ['darwin-service:8080']
#+end_src
#+begin_src txt :tangle ./prometheus/darwin-prometheus-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: darwin-prometheus-service
  namespace: darwin
spec:
  type: NodePort
  selector:
    app.kubernetes.io/name: prometheus
  ports:
    - name: web
      port: 9090
      targetPort: 9090
      nodePort: 30000
#+end_src
**** Configuração
*** Instalação de Grafana
