#+TITLE: Roteiro Kubernetes

* Introdução
Essa documentação utiliza princípios de programação
letrada. Consequentemente, alguns snippets que são descritos abaixo então
contidos em scripts extraídos da documentação. O nome do script
que contém o snippet respectivo está em um comentário na primeira
linha do snippet. 

Todos os scripts e comandos devem
ser executados como usuário raiz.

O login de usuário root via SSH deve
estar habilitado também.
* Clonar VMs
A clonagem das VMs é feita através do ProxMox manualmente.
* SSH
Criação de chaves assimétricas para acesso SSH sem senha. Esse passo deve ser
feito manualmente.
** Cria chaves assimétricas
#+begin_src sh
  # A variável HOSTS deve conter os IPs dos hosts da instalação do cluster.
  # Exemplo: HOSTS=(10.20.1.123 10.20.1.124)
  HOSTS=()
  ssh-keygen -q -t rsa -b 4096 -f ~/.ssh/id_rsa -P ""
  for HOST in "${HOSTS[@]}"; do ssh-copy-id $USER@$HOST; done
#+end_src
* Instalar Python
Instalação de Python e Pip no host de onde partirá a instalação
do kubernetes. O seguinte snippet instala Python e Pip em sistemas
baseados em RPM. Caso o sistema ponto de partida não seja baseado em
RPM, deve-se utilizar os comandos cabíveis.
#+begin_src sh :tangle deps-install.sh
  # ./deps-install.sh
  sudo dnf install python39 python3-pip -y
  pip3 install --upgrade pip
#+end_src
* Clonar repositório do Kubespray e instalar dependências.
Caso a instalação esteja partindo de uma máquina exterior ao cluster,
pode-se utilizar o seguinte script; mas se a instalação estiver
partindo de um dos nós do cluster, deve-se omitir a instalação do do
ambiente virtual Python.

O script abaixo instala as dependências Python do Kubespray no
diretório /tmp/repos/kubespray-venv. Segundo a [[https://docs.python.org/3/library/venv.html][documentação]] do Python,
esses ambientes virtuais servem para resolver problemas de
incompatibilidade entre versões de um mesmo software requerido em
várias versões. Em alguns dos scripts/snippets que seguem,
utilizaremos essas dependências por conveniência.
#+begin_src sh :tangle kubespray-install.sh
	# ./kubespray-install.sh
	if [ ! -d ./kubespray ]; then
		git clone https://github.com/kubernetes-sigs/kubespray
	fi

	EXTERNAL=""
	read -p "A instalação do cluster parte de uma máquina externa ao cluster? (y/N)" EXTERNAL
	if [ $EXTERNAL = "y" ]; then
	  pip install virtualenv

	  VENVDIR=kubespray-venv
	  KUBESPRAYDIR=kubespray
	  ANSIBLE_VERSION=2.12
	  virtualenv  --python=$(which python3) $VENVDIR
	  source $VENVDIR/bin/activate
	  cd $KUBESPRAYDIR
	  pip install -U -r requirements-$ANSIBLE_VERSION.txt
	elif [ $EXTERNAL ="N" ]; then 
	  cd kubespray
	  pip install -U -r requirements.txt
	else
	  echo "Entrada inválida"
	fi
  #+end_src
* Atualizar sistemas
Caso não tenha sido feito ou precisa fazer novamente, atualizar os
sistemas operacionais. Esse script depende da existência dos arquivos
"inventory.ini" e "update-systems-playbook.yaml" que são providos
nesse repositório.
#+begin_src sh :tangle update-systems.sh
  # ./update-systems.sh
  (
	  source ./kubespray-venv/bin/activate
	  ansible-playbook -i inventory.ini update-systems-playbook.yaml --become --become-user=root
  )
#+end_src
Playbook Ansible que especifica a tarefa para atualizar os pacotes do
sistema operacional alvo para as versões mais recentes da distribuição.
#+begin_src yml :tangle update-systems-playbook.yaml
- name: Atualiza sistema
  hosts: server*
  become: true
  tasks:
  - name: Atualiza sistema
    package:
      name: '*'
      state: latest
#+end_src
Inventário contendo configuração para acesso aos hosts. Esse arquivo
deve ser atualizado com os IPs dos hosts do cluster manualmente.
#+begin_src text :tangle inventory.ini
server1 ansible_host=10.20.1.113
server2 ansible_host=10.20.1.115
#+end_src
* Desabilitar firewall
Caso os firewalls não tenham sido desabilitados, desabilitá-los.
#+begin_src sh :tangle remove-firewall.sh
  # ./remove-firewall.sh
  (
	  source ./kubespray-venv/bin/activate
	  ansible-playbook -i inventory.ini remove-firewall-playbook.yaml --become --become-user=root
  )
#+end_src
#+begin_src yml :tangle remove-firewall-playbook.yaml
- name: Remove firewall e habilitar login de root via SSH
  hosts: server*
  become: true
  tasks:
  - name: Remove firewall
    shell: |
      systemctl stop firewalld.service
      systemctl disable firewalld.service
#+end_src
* Próximos passos
Próximos passos são documentados no repositório oficial do [[https://github.com/kubernetes-sigs/kubespray][Kubespray]].
Mas há algumas ressalvas. A instalação utilizando ambientes virtuais
do python quando a instalação é feita a partir de uma máquina que será
um nó do cluster apresenta erros na busca de dependências do python.
Caso a máquina de partida da instalação deva ser um nó esse passo pode
ser omitido de qualquer modo.

Os seguintes passos, que são os mais importantes, devem ser executados manualmente:
#+begin_src sh
  # Na raíz do projeto kubespray.
  cd kubespray
  
  cp -rfp inventory/sample inventory/mycluster
  # IPS é um vetor contendo os IPs dos hosts do cluster.
  declare -a IPS=()

  # A seguinte linha deve ser executada somente se a máquina de onde parte
  # a instalação for externa ao cluster.
  source ../kubespray-venv/bin/activate

  # Esse script gera o inventário automaticamente com configuração padrão.
  CONFIG_FILE=inventory/mycluster/hosts.yaml python3 contrib/inventory_builder/inventory.py ${IPS[@]}
  # Nesse ponto pode-se revisar e modificar as variáveis em
  # inventory/mycluster/group_vars/all/all.yml e
  # inventory/mycluster/group_vars/k8s_cluster/k8s_cluster.yml.

  # Para limpar um cluster velho, executar como root:
  ansible-playbook -i inventory/mycluster/hosts.yaml --become --become-user=root reset.yml
  # Para fazer uma nova instalação do kubernetes, executar como root:
  ansible-playbook -i inventory/mycluster/hosts.yaml --become --become-user=root cluster.yml
#+end_src
* Instalação do Wordpress + MySQL
Os passos utilizados para instalação do Wordpress e MySQL consistem
na aplicação de um [[https://kubernetes.io/docs/concepts/workloads/controllers/deployment/][deployment]] e de um [[https://kubernetes.io/docs/concepts/services-networking/service/][service]] para cada
componente. Cada deployment possuirá também uma configuração para
utilizar um servidor NFS como armazenamento persistente, com o intúito
de preservar a configuração das aplicações e e dos dados do banco de dados entre
possíveis deployments (ex.: um nó é desligo ou cai).
Também possuirá uma configuração que especifica uma [[https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/][toleration]], i.e.,
um espaço de tempo que um container permanecerá atrelado a um nó
enquanto uma taint for verificada, por exemplo, quando a taint
not-ready estiver verificada quando o nó estiver fora do ar.

** Instalação do servidor NFS
Antes de tudo, executamos o seguinte roteiro para criar um servidor
NFS. No host onde o servidor NFS será hospedado, executar, como raiz:
#+begin_src sh
  # Como raiz.
  dnf install nfs-utils -y
  mkdir /var/nfs/general -p
  touch /etc/exports
  
  # Colocar IPs dos workers no vetor HOSTS.
  # Exemplo: HOSTS=(10.20.1.113 10.20.1.118)
  export HOSTS=()
  # Fazemos export para podermos utilizar essa variável em outros momentos, caso cabível.
  for i in "${HOSTS[@]}"; do echo "/var/nfs/general $i(rw,no_subtree_check,no_root_squash)" >> /etc/exports;done

  systemctl enable nfs-server
  systemctl start nfs-server

  # Esse comando deve ser executado toda vez que o arquivo /etc/exports
  # for modificado.
  exportfs -ra

  # Os hosts clientes também precisam do pacote nfs-utils, caso não estejam instalados
  # então instalamos ele:
  for i in "${HOSTS[@]}"; do ssh $USER@$i "dnf install nfs-utils -y";done
#+end_src
Caso o servidor NFS já exista, deve-se executar apenas os seguintes comandos no servidor:
#+begin_src sh
  # Modificar manualemente o arquivo /etc/exports
  # ou então executar o seguinte snippet.
  HOSTS=()
  for i in "${HOSTS[@]}"; do echo "/var/nfs/general $i(rw,no_subtree_check,no_root_squash)" >> /etc/exports;done

  exportfs -ra
#+end_src
Os comandos acima especificam o diretório a ser montado nos clientes,
os IPS dos clientes e configurações por IP.

Deve-se também criar pastas específicas de cada aplicação no diretório
/var/nfs/general/
(ex.: /var/nfs/general/mysql-igor)
e deixá-las com permissão 777 para evitar erros de permissão e também
com usuário e grupo nobody.
#+begin_src sh
  chmod 777 -R /var/nfs/general/<DIR>
  chown nobody:nobody -R /var/nfs/general/<DIR>
#+end_src
** Cópia dos arquivos de configuração para o cluster
Copie os arquivos de configuração mysql-dep.yml, mysql-serv.yml,
wordpress-dep.yml e wordpress-serv.yml para um master do cluster utilizando o
comando, na raíz do projeto:
#+begin_src sh
  scp wordpress/*.yml root@<HOST-IP>
#+end_src
Onde HOST-IP é o IP de um dos control_planes do cluster.
** Aplicação do Deployment do MySQL
Logado em um dos master nodes (control_planes) modificar o seguinte
arquivo de configuração para servir suas necessidades, como o caminho
para o diretório dos arquivos da aplicação no servidor NFS.
#+begin_src txt :tangle wordpress/mysql-dep.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mysql-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql:latest
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: password
        ports:
        - containerPort: 3306
        volumeMounts:
        - name: nfs-volume
          mountPath: /var/lib/mysql
      volumes:
      - name: nfs-volume
        nfs:
          server: 10.20.1.111
          path: /var/nfs/general/mysql-igor
          readOnly: no
      tolerations:
      - effect: NoExecute
        key: node.kubernetes.io/not-ready
        operator: Exists
        tolerationSeconds: 30
      - effect: NoExecute
        key: node.kubernetes.io/unreachable
        operator: Exists
        tolerationSeconds: 30
#+end_src
Depois execute o seguinte comando para levantar o deployment do MySQL.
#+begin_src sh
  kubectl apply -f mysql-dep.yml
#+end_src
O seguintes comandos podem ser utilizados para resgatar informações básicas
sobre o deployment e sobre o pod criado.
#+begin_src sh
  kubectl get deployment -o wide
  kubectl get pod -o wide
#+end_src
** Aplicação do Service do MySQL
O seguinte arquivo configura o serviço para o MySQL. Caso queira, pode
modificar a porta de acesso externo serviço do pod modificando o campo
"targetPort".
#+begin_src txt :tangle wordpress/mysql-serv.yml
apiVersion: v1
kind: Service
metadata:
  name: mysql-service
spec:
  selector:
    app: mysql
  ports:
    - protocol: TCP
      port: 3306
      targetPort: 3306
#+end_src
Utilize o seguinte comando para aplicar a configuração do serviço MySQL.
#+begin_src sh
  kubectl apply -f mysql-serv.yml
#+end_src
O seguinte comando pode ser utilizado para resgatar informações básicas
sobre o Service criado.
#+begin_src sh
  kubectl get svc -o wide
#+end_src
** Aplicação do Deployment do Wordpress
Novamente, revise o seguinte arquivo de configuração do deployment
para o Wordpress e modifique os campos que forem necessários, como o
para os arquivos específicos do Wordpress no servidor NFS.
#+begin_src txt :tangle wordpress/wordpress-dep.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: wordpress-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: wordpress
  template:
    metadata:
      labels:
        app: wordpress
    spec:
      containers:
      - name: wordpress
        image: wordpress:latest
        env:
        - name: WORDPRESS_DB_HOST
          value: mysql-service
        - name: WORDPRESS_DB_USER
          value: root
        - name: WORDPRESS_DB_PASSWORD
          value: password
        - name: WORDPRESS_DB_NAME
          value: wordpress
        ports:
        - containerPort: 80
        volumeMounts:
        - name: nfs-volume
          mountPath: /var/www/html
      volumes:
      - name: nfs-volume
        nfs:
          server: 10.20.1.111
          path: /var/nfs/general/wordpress-igor
          readOnly: no
      tolerations:
      - effect: NoExecute
        key: node.kubernetes.io/not-ready
        operator: Exists
        tolerationSeconds: 30
      - effect: NoExecute
        key: node.kubernetes.io/unreachable
        operator: Exists
        tolerationSeconds: 30
#+end_src
Utilize o seguinte comando para aplicar o deployment do Wordpress.
#+begin_src sh
  kubectl apply -f wordpress-dep.yml
#+end_src
** Aplicação do Service do Wordpress
Revise o arquivo de configuração do serviço Wordpress e modifique os
campos que achar necessário.
#+begin_src txt :tangle wordpress/wordpress-serv.yml
xuapiVersion: v1
kind: Service
metadata:
  name: wordpress-service
spec:
  selector:
    app: wordpress
  type: NodePort
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
      nodePort: 30036 
#+end_src
O seguinte comando aplica a configuração do serviço Wordpress.
#+begin_src sh
  kubectl apply -f wordpress-serv.yml
#+end_src
Os seguintes comandos podem ser utilizados para fazer troubleshooting
nos respectivos componentes Kubernetes:
#+begin_src sh
  kubectl describe deployment
  kubectl describe pod
  kubectl describe service
  kubectl logs <POD_NAME>
#+end_src
** Criação do banco de dados MySQL no container
A partir de control_plane logar no container:
#+begin_src sh
  kubectl get pods
  kubectl exec -it <MYSQL_POD_NAME> -- bash
  mysql -u root -p
  # no prompt do shell do mysql:
  create database wordpress;
  exit
  exit
#+end_src
* Instalação do Gitlab
** Aplicação do Deployment para o Gitlab
Revise o arquivo de configuração do deployment para o Gitlab e edite
os campos necessários, como os caminhos nos volumes "gitlab-data",
"gitlab-logs" e "gitlab-config" para servir a sua configuração. Vale
ressaltar que os caminhos para esses volumes devem ser diferentes.
#+begin_src txt :tangle gitlab/gitlab-dep.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gitlab-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: gitlab
  template:
    metadata:
      labels:
        app: gitlab
    spec:
      containers:
      - name: gitlab
        image: gitlab/gitlab-ce:latest
        env:
        - name: GITLAB_OMNIBUS_CONFIG
          value: |
            external_url 'http://localhost'
        ports:
        - containerPort: 80
        volumeMounts:
        - name: gitlab-data
          mountPath: /var/opt/gitlab
        - name: gitlab-logs
          mountPath: /var/log/gitlab
        - name: gitlab-config
          mountPath: /etc/gitlab
      volumes:
      - name: gitlab-data
        nfs:
          server: 10.20.9.111
          path: /var/nfs/general/gitlab-igor/data
          readOnly: no
      - name: gitlab-logs
        nfs:
          server: 10.20.9.111
          path: /var/nfs/general/gitlab-igor/logs
          readOnly: no
      - name: gitlab-config
        nfs:
          server: 10.20.9.111
          path: /var/nfs/general/gitlab-igor/config
          readOnly: no
      tolerations:
      - effect: NoExecute
        key: node.kubernetes.io/not-ready
        operator: Exists
        tolerationSeconds: 30
      - effect: NoExecute
        key: node.kubernetes.io/unreachable
        operator: Exists
        tolerationSeconds: 30
#+end_src
Execute o seguinte comando para aplicar a configuração do deployment:
#+begin_src sh
  kubectl apply -f gitlab-dep.yml
#+end_src
** Aplicação do Service para o Gitlab
Revise o arquivo de configuração para o serviço do Gitlab e edite o
que achar necessário.
#+begin_src txt :tangle gitlab/gitlab-serv.yml
apiVersion: v1
kind: Service
metadata:
  name: gitlab-service
spec:
  selector:
    app: gitlab
  type: NodePort
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
      nodePort: 30036 
#+end_src
Execute o seguinte comando para aplicar a configuração:
#+begin_src sh
  kubectl apply -f gitlab-serv.yml
#+end_src
* Instalação de um Runner no Gitlab
** Instalação do Docker
#+begin_src sh :tangle ./install-docker.sh
  #./install-docker.sh
  dnf upgrade --refresh -y
  dnf config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
  dnf install docker-ce docker-ce-cli containerd.io
  systemctl start docker
#+end_src
** Logar numa instância Docker do Gitlab Runner
#+begin_src
docker run -it --entrypoint /bin/bash gitlab/gitlab-runner:latest
#+end_src
** Registrar o Gitlab Runner
*** Gerar um token para registrar o Gitlab Runner
Crie um repositório teste na sua instância do Gitlab. Acesse o
repositório teste e na tela do repositório vá em Settings -> CI/CD ->
Runners e siga as instruções para registrar um novo runner. O runner
no nosso caso deve utilizar a plataforma Linux e deve ser configurado
para executar trabalhados sem tag acionando o checkbox "Run untagged
jobs".

Você será direcionado para uma tela onde constará o token gerado.
*** Registrar
Volte para o shell logado no container Docker e execute:
#+begin_src sh
gitlab-runner register --url <CAMINHO_PARA_O_CLUSTER> --token <TOKEN_GERADO>
#+end_src
O CAMINHO_PARA_O_CLUSTER é o IP para qualquer nó do cluster (ex.:
http://10.20.9.116:30036).
Você pode visualizar o arquivo de configuração, gerado
automaticamente, do Runner com o comando:
#+begin_src s
more /etc/gitlab-runner/config.toml
#+end_src
Nós vamos utilizar esse arquivo para configurar o nosso container
Kubernetes do Gitlab Runner. No meu caso o arquivo é estruturado
assim:
#+begin_src txt :tangle ./gitlab/config.toml
concurrent = 1
check_interval = 0
shutdown_timeout = 0

[session_server]
  session_timeout = 1800

[[runners]]
  name = "runner"
  url = "http://10.20.9.116:30036"
  id = 1
  token = "glrt-qJDS_pTGimZC8YtaoBPw"
  token_obtained_at = 2023-05-31T16:40:36Z
  token_expires_at = 0001-01-01T00:00:00Z
  executor = "docker"
  [runners.cache]
    MaxUploadedArchiveSize = 0
  [runners.docker]
    tls_verify = false
    image = "busybox:latest"
    privileged = false
    disable_entrypoint_overwrite = false
    oom_kill_disable = false
    disable_cache = false
    volumes = ["/cache"]
    shm_size =0 
#+end_src
Agora podemos deslogar do container Docker e derrubar o serviço Docker
e proceder para os próximos passos.
#+begin_src sh
systemctl stop docker
#+end_src
*** Configurar o Gitlab Runner do Kubernetes
Para configurar o Gitlab Runner, precisamos adicinoar papéis de
autenticação para o cluster Kubernetes ao runner. O seguinte
arquivo provê essa configuração:
#+begin_src txt :tangle ./gitlab/gitlab-runner-authentication.yml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: gitlab-admin
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: gitlab-admin
rules:
  - apiGroups: [""]
    resources: ["*"]
    verbs: ["*"]

---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: gitlab-admin
subjects:
  - kind: ServiceAccount
    name: gitlab-admin
roleRef:
  kind: Role
  name: gitlab-admin
  apiGroup: rbac.authorization.k8s.io
#+end_src
Aplique essa configuração com o comando:
#+begin_src sh
kubectl apply -f gitlab-runner-authentication.yml
#+end_src
Depois de termos um ServiceAccount, Role e RoleBinding configurados
precisamos de um ConfigMap para persistir a configuração do runner.
O arquivo de configuração do ConfigMap deve especificar o arquivo de
configuração do runner que roubamos do container docker com algumas
modificações para adaptá-lo ao ambinete Kubernetes. No meu caso,
ficou assim:
#+begin_src txt :tangle ./gitlab/gitlab-runner-config.yml
apiVersion: v1
kind: ConfigMap
metadata:
  name: gitlab-runner-config
data:
  config.toml: |-
     concurrent = 4
     [[runners]]
       name = "runner"
       url = "http://10.20.9.116:30036"
       id = 1
       token = "glrt-qJDS_pTGimZC8YtaoBPw"
       token_obtained_at = 2023-05-31T16:40:36Z
       token_expires_at = 0001-01-01T00:00:00Z
       executor = "kubernetes"
       [runners.kubernetes]
          poll_timeout = 600
          cpu_request = "1"
          service_cpu_request = "200m"
#+end_src
Após a aplicação dessa configuração, aplicamos a configuração do
Deployment:
#+begin_src txt :tangle ./gitlab/gitlab-runner-deployment.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gitlab-runner
spec:
  replicas: 1
  selector:
    matchLabels:
      name: gitlab-runner
  template:
    metadata:
      labels:
        name: gitlab-runner
    spec:
      serviceAccountName: gitlab-admin
      containers:
        - args:
          - run
          image: gitlab/gitlab-runner:latest
          imagePullPolicy: Always
          name: gitlab-runner
          resources:
            requests:
              cpu: "100m"
            limits:
              cpu: "100m"
          volumeMounts:
            - name: config
              mountPath: /etc/gitlab-runner/config.toml
              readOnly: true
              subPath: config.toml
      volumes:
        - name: config
          configMap:
            name: gitlab-runner-config
      restartPolicy: Always
#+end_src
*** Verificar a instalação do runner
Você pode verificar se a instalação do runner obteve sucesso indo para
a tela Settings -> CI/CD -> Runners e checando se há um runner verde
na seção "Assigned project runners".
* Problemas encontrados
** 04-24-2023, 14:12
-  Após a instalação com sucesso houve algumas falhas: 1 nó
   configurado como control_plane não consta como control_plano na
   saída do comando "kubectl get node".
-  Após a instalação 2 nós constam como status "NotReady".
-  Os passos para chegar nessa situação foram:
   + Primeiro foi feita uma tentativa de instalação com inventário
     manualmente escrito. Nessa tentativa houve 1 único erro em todos
     os nós que dizia respeito ao serviço de firewall do SO.
   + Depois foi feita uma tentativa com um inventário escrito
     automaticamente pelo script como está na documentação do
     Kubespray. O script não modificou coisas relevantes no
     inventário. Nessa instalação houveram erros que diziam respeito
     ao nome de módulos de kernel, mas o sumário ao fim da instalação
     não constavam como se a instalação tivesse sido prejudicada por isso.
** 04-25-2023, 12:00
- Instalação do Kubernetes utilizando ambientes virtuais do python
  apresenta erros quando a máquina de onde parte a instalação será um
  nó do cluster. Com a utilização dos ambientes virtuais do python, o
  interpretador python utilizado é o que está dentro do ambiente
  Virtual. O ansible não consegue encontrar o módulo selinux-python,
  nesse caso, mas consegue encontrar caso utilizemos o ambiente global
  do python.

* TODO Correções
- Corrigir URL para instância Gitlab
