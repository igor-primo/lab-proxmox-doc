#+TITLE: Roteiro Kubernetes

* Introdução
Essa documentação utiliza princípios de programação
letrada. Consequentemente, alguns snippets que são descritos abaixo então
contidos em scripts extraídos da documentação. O nome do script
que contém o snippet respectivo está em um comentário na primeira
linha do snippet. 

Todos os scripts e comandos devem
ser executados como usuário raiz. Recomenda-se que se utilize uma
máquina exterior ao cluster para fazer a instalação.

O login de usuário root via SSH deve
estar habilitado também.
* Clonar VMs
A clonagem das VMs é feita através do ProxMox manualmente.
* SSH
Criação de chaves assimétricas para acesso SSH sem senha. Esse passo deve ser
feito manualmente.
** Criação das chaves assimétricas
#+begin_src sh
  # A variável HOSTS deve conter os IPs dos hosts da instalação do cluster
  # separados por um espaço.
  # Exemplo: HOSTS=(10.20.1.123 10.20.1.124)
  HOSTS=()
  # Gera uma chave RSA
  # Explicação para as bandeiras para o programa ssh-keygen
  # -q: modo silencioso
  # -t: especifica o tipo de chave a ser criada
  # -b: especifica o número de bits na chave a ser criada
  # -f: especifica o nome do arquivo do arquivo da chave
  # -P: especifica a senha antiga
  ssh-keygen -q -t rsa -b 4096 -f ~/.ssh/id_rsa -P ""
  # Copia a chave pública correspondente para os hospedeiros remotos
  for HOST in "${HOSTS[@]}"; do ssh-copy-id $USER@$HOST; done
#+end_src
* Instalar Python
Instalação de Python e Pip no host de onde partirá a instalação
do kubernetes. O seguinte snippet instala Python e Pip em sistemas
baseados em RPM. Caso o sistema ponto de partida não seja baseado em
RPM, deve-se utilizar os comandos cabíveis.
#+begin_src sh :tangle deps-install.sh :shebang "#!/bin/bash"
  # ./deps-install.sh
  sudo dnf install python39 python3-pip -y
  pip3 install --upgrade pip
#+end_src
* Clonar repositório do Kubespray e instalar dependências.
Caso a instalação esteja partindo de uma máquina exterior ao cluster,
pode-se utilizar o seguinte script; mas se a instalação estiver
partindo de um dos nós do cluster, deve-se omitir a instalação do
ambiente virtual Python.

O script abaixo instala as dependências Python do Kubespray no
diretório ./kubespray-venv. Segundo a [[https://docs.python.org/3/library/venv.html][documentação]] do Python,
esses ambientes virtuais servem para resolver problemas de
incompatibilidade entre versões de um mesmo software requerido em
várias versões. Em alguns dos scripts/snippets que seguem,
utilizaremos essas dependências por conveniência.
#+begin_src sh :tangle kubespray-install.sh :shebang "#!/bin/bash"
  # ./kubespray-install.sh
  if [ ! -d ./kubespray-2.22.0 ]; then
	  wget https://github.com/kubernetes-sigs/kubespray/archive/refs/tags/v2.22.0.tar.gz
	  tar -xf v2.22.0.tar.gz
	  rm v2.22.0.tar.gz
  fi

  EXTERNAL=""
  read -p "A instalação do cluster parte de uma máquina externa ao cluster? (y/N)" EXTERNAL
  if [ $EXTERNAL = "y" ]; then
	pip install virtualenv
	VENVDIR=kubespray-venv
	KUBESPRAYDIR=kubespray-2.22.0
	ANSIBLE_VERSION=2.12
	virtualenv  --python=$(which python3) $VENVDIR
	source $VENVDIR/bin/activate
	cd $KUBESPRAYDIR
	pip install -U -r requirements-$ANSIBLE_VERSION.txt
  elif [ $EXTERNAL = "N" ]; then 
	cd kubespray-2.22.0
	pip install -U -r requirements.txt
  else
	echo "Entrada inválida"
  fi
  #+end_src
* Atualizar sistemas
Caso julgue necessário, atualizar os sistemas operacionais.
Esse script depende da existência dos arquivos
"inventory.ini" e "update-systems-playbook.yaml" que são providos
nesse repositório.

Esse é playbook Ansible que especifica a tarefa para atualizar os pacotes do
sistema operacional alvo para as versões mais recentes da
distribuição. Não precisa ser modificado.
#+begin_src yml :tangle update-systems-playbook.yaml
- name: Atualiza sistema
  hosts: server*
  become: true
  tasks:
  - name: Atualiza sistema
    package:
      name: '*'
      state: latest
#+end_src
Esse é o inventário contendo configuração para acesso aos hosts. Esse arquivo
deve ser atualizado com os IPs dos hosts do cluster manualmente.
#+begin_src text :tangle inventory.ini
server1 ansible_host=10.20.1.113
server2 ansible_host=10.20.1.115
#+end_src
Esse é o mini-roteiro a ser utilizado para aplicar o YAML anterior.
#+begin_src sh :tangle update-systems.sh :shebang "#!/bin/bash"
  # ./update-systems.sh
  (
	  # Se o diretório ./kubespray-venv existe, assume que
	  # a instalação parte de uma máquina exterior ao cluster
	  # e carrega as variáveis do ambiente virtual
	  if [ -d ./kubespray-venv ]; then
		  source ./kubespray-venv/bin/activate
	  fi
	  ansible-playbook -i inventory.ini update-systems-playbook.yaml --become --become-user=root
  )
#+end_src
* Desabilitar firewall
Caso os firewalls não tenham sido desabilitados, desabilitá-los.

Esse é o playbook Ansible utilizado para instruir o Ansible para
executar os comandos para parar e desabilitar o serviço de firewall
nas máquinas alvo.

Esse playbook utiliza o inventário mencionado na seção anterior,
portanto ele deve estar atualizado com os IPs das máquianas alvo.
#+begin_src yml :tangle remove-firewall-playbook.yaml 
- name: Remove firewall e habilitar login de root via SSH
  hosts: server*
  become: true
  tasks:
  - name: Remove firewall
    shell: |
      systemctl stop firewalld.service
      systemctl disable firewalld.service
#+end_src
Esse é o snippet para aplicar a configuração.
#+begin_src sh :tangle remove-firewall.sh :shebang "#!/bin/bash"
  # ./remove-firewall.sh
  (
	  # Se o diretório ./kubespray-venv existe, assuma que
	  # a instalação parte de uma máquina exterior ao cluster
	  # e carregue as variáveis do ambiente virtual
	  if [ -d ./kubespray-venv ]; then
		  source ./kubespray-venv/bin/activate
	  fi
	  ansible-playbook -i inventory.ini remove-firewall-playbook.yaml --become --become-user=root
  )
#+end_src
* Instalação do Kubernetes
A instalação do Kubernetes pode ser feita seguindo o seguinte snippet.
Mas há algumas ressalvas. A instalação utilizando ambientes virtuais
do Python quando a instalação é feita a partir de uma máquina que será
um nó do cluster apresenta erros na busca de dependências do python.
Mas como caso a máquina deva ser parte do cluster esse roteiro instrui
para não utilizar ambientes virtuais do Python para instalar as
dependências do Kubespray, esse problema já foi contornado.

Os seguintes passos, que são os mais importantes, devem ser executados manualmente:
#+begin_src sh
  # Na raíz do projeto kubespray.
  cd kubespray-2.22.0
  
  cp -rfp inventory/sample inventory/mycluster
  # IPS é um vetor contendo os IPs dos hosts do cluster.
  declare -a IPS=()

  # A seguinte linha deve ser executada somente se a máquina de onde parte
  # a instalação for externa ao cluster.
  source ../kubespray-venv/bin/activate

  # Esse script gera o inventário automaticamente com configuração padrão.
  CONFIG_FILE=inventory/mycluster/hosts.yaml python3 contrib/inventory_builder/inventory.py ${IPS[@]}
  # Nesse ponto pode-se revisar e modificar as variáveis em
  # inventory/mycluster/group_vars/all/all.yml e
  # inventory/mycluster/group_vars/k8s_cluster/k8s_cluster.yml.

  # Para limpar um cluster velho, executar como root:
  ansible-playbook -i inventory/mycluster/hosts.yaml --become --become-user=root reset.yml
  # Para fazer uma nova instalação do kubernetes, executar como root:
  ansible-playbook -i inventory/mycluster/hosts.yaml --become --become-user=root cluster.yml
#+end_src
Mais detalhes são documentados no repositório oficial do [[https://github.com/kubernetes-sigs/kubespray][Kubespray]].
* Instalação do Wordpress + MySQL
Terminologia:
- Node :: Máquinas físicas ou virtuais que executam Pods.
- Pod :: Um Pod é um grupo de um ou mais contêineres, com
  armazenamento e recursos de rede compartilhados, e uma
  especificação de como executar os contêineres.
- Deployment :: Componente que fornece atualizações declarativas para
  Pods e ReplicaSets. O propósito de um ReplicaSet é manter um
  conjunto estável de réplicas de Pods executando em um dado
  momento. O ReplicaSet cumpre o seu propósito criando e deletando
  Pods quando necessário para atingir o número desejado de Pods. O
  Deployment cria ReplicaSets que criam Pods replicados.
- Service :: É um método para expor uma aplicação de rede que está
  sendo exectuado como um ou mais Pods no cluster. É necessário porque
  apesar de um IP ser atribuído a um Pod pelos plugins de rede nativos
  do Kubernetes, Pods são componentes efêmeros, podendo ser destruídos
  e reconstruídos por Deployments, o que pode modificar seus
  IPs. Services servem para atrelar um IP fixo de acesso aos serviços nos
  Pods.
- Toleration e Taint :: "Tolerations" e "Taints" são termos
  relacionados ao conceito de afinidade de nó, que é uma propriedade
  de Pods que os "atrai" para um conjunto de nós, seja como
  preferência ou exclusividade. Taints são propriedades de Pods que os
  repelem de um conjunto de nós. Tolerations especificam condições de
  tolerância para a permanência de um Pod em um nó. Pode-se
  especificar condições de memória, processamento ou rede. Quando
  essas condições de tolerância são ultrapassadas, o Pod possuindo a
  tolerância é reagendado para outro nó.

Os passos utilizados para instalação do Wordpress e MySQL consistem
na aplicação de um [[https://kubernetes.io/docs/concepts/workloads/controllers/deployment/][deployment]] e de um [[https://kubernetes.io/docs/concepts/services-networking/service/][service]] para cada
componente. Cada deployment possuirá também uma configuração para
utilizar um servidor NFS como armazenamento persistente, com o intuito
de preservar a configuração das aplicações e dos dados do banco de dados entre
possíveis deployments (ex.: um nó é desligado ou cai).
Também possuirá uma configuração que especifica uma [[https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/][toleration]], nesse caso,
um espaço de tempo que um container permanecerá atrelado a um nó
enquanto uma taint for verificada, por exemplo, quando a taint
not-ready estiver verificada quando o nó estiver fora do ar.

Para mais detalhes sobre o que esses termos significam, vide a documentação
referenciada nos links nesse parágrafo.
** Instalação do servidor NFS
Antes de tudo, executamos o seguinte roteiro para criar um servidor
NFS. No host onde o servidor NFS será hospedado, executar, como raiz:
#+begin_src sh
  # Como raiz.
  dnf install nfs-utils -y
  mkdir /var/nfs/general -p
  touch /etc/exports

  # Colocar IPs dos workers no vetor HOSTS.
  # Exemplo: HOSTS=(10.20.1.113 10.20.1.118)
  # Exportamos a variável para podermos utilizar essa variável em outros momentos, caso cabível.
  export HOSTS=()

  # O seguinte laço estabelece o ponto de montagem com permissões e configurações para o IP respectivo.
  # rw: permissões de leitura e escrita
  # no_subtree_check: desativa checagens que o servidor faz para ter certeza de que o cliente está acessando
  #   um arquivo/diretório dentro do diretório exportado. Melhora performance.
  # no_root_squash: permite que o cliente root leia e escreva arquivos como usuário root
  for i in "${HOSTS[@]}"; do echo "/var/nfs/general $i(rw,no_subtree_check,no_root_squash)" >> /etc/exports;done

  systemctl enable nfs-server
  systemctl start nfs-server

  # Esse comando deve ser executado toda vez que o arquivo /etc/exports
  # for modificado.
  exportfs -ra

  # Os hosts clientes também precisam do pacote nfs-utils, caso não estejam instalados
  # então instalamos ele:
  for i in "${HOSTS[@]}"; do ssh $USER@$i "dnf install nfs-utils -y";done
#+end_src
Caso o servidor NFS já exista, deve-se executar apenas os seguintes comandos no servidor:
#+begin_src sh
  # Modificar manualemente o arquivo /etc/exports
  # ou então executar o seguinte snippet.
  HOSTS=()
  for i in "${HOSTS[@]}"; do echo "/var/nfs/general $i(rw,no_subtree_check,no_root_squash)" >> /etc/exports;done

  exportfs -ra
#+end_src
Os comandos acima especificam o diretório a ser montado nos clientes,
os IPS dos clientes e configurações por IP.

Deve-se também criar pastas específicas de cada aplicação no diretório
/var/nfs/general/
(ex.: /var/nfs/general/mysql-igor)
e deixá-las com permissão 777 para evitar erros de permissão e também
com usuário e grupo nobody. Os comandos são esses, por exemplo:
#+begin_src sh
  chmod -R 777 /var/nfs/general/mysql-igor
  chown -R nobody:nobody /var/nfs/general/mysql-igor
#+end_src
** Cópia dos arquivos de configuração para o cluster
Copie os arquivos de configuração mysql-dep.yml, mysql-serv.yml,
wordpress-dep.yml e wordpress-serv.yml para um master do cluster utilizando o
comando, na raíz do projeto:
#+begin_src sh
  scp wordpress/*.yml root@<HOST-IP>
#+end_src
Onde HOST-IP é o IP de um dos control_planes do cluster.
** Troubleshooting
Para fazer troubleshooting, visualizar logs e informações sobre as
ações do kubernetes pode-se utilizar esses comandos:
#+begin_src sh
  # Lista deployments
  kubectl get deployments -o wide

  # Lista pods
  kubectl get pods -o wide

  # Lista serviços
  kubectl get svc -o wide

  # Visualiza detalhes sobre um recurso ou grupo de recursos específico
  kubectl describe deployments
  kubectl describe deployment <DEPLOYMENT_NAME>

  # Visualiza logs emitidos por um pod
  kubectl logs --follow <POD_NAME>

  # Para ver os detalhes de todos os comandos possívels
  kubectl --help | less
#+end_src
Para mais detalhes sobre as possibilidades de comandos, vide o
[[https://kubernetes.io/pt-br/docs/reference/kubectl/cheatsheet/][cheat sheet]].
** Aplicação do Deployment do MySQL
Logado em um dos master nodes (control_planes) modificar o seguinte
arquivo de configuração para servir suas necessidades, como o caminho
para o diretório dos arquivos da aplicação no servidor NFS.
#+begin_src txt :tangle wordpress/mysql-dep.yml
# Arquivo: ./wordpress/mysql-dep.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mysql-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql:latest
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: password
        ports:
        - containerPort: 3306
        volumeMounts:
        - name: nfs-volume
          mountPath: /var/lib/mysql
      volumes:
      - name: nfs-volume
        nfs:
          server: 10.20.1.111
          path: /var/nfs/general/mysql-igor
          readOnly: no
      tolerations:
      - effect: NoExecute
        key: node.kubernetes.io/not-ready
        operator: Exists
        tolerationSeconds: 30
      - effect: NoExecute
        key: node.kubernetes.io/unreachable
        operator: Exists
        tolerationSeconds: 30
#+end_src
Depois execute o seguinte comando para levantar o deployment do MySQL.
#+begin_src sh
  kubectl apply -f mysql-dep.yml
#+end_src
** Aplicação do Service do MySQL
O seguinte arquivo configura o serviço para o MySQL. Caso queira, pode
modificar a porta de acesso externo serviço do Pod modificando o campo
"targetPort".
#+begin_src txt :tangle wordpress/mysql-serv.yml
# Arquivo: ./wordpress/mysql-serv.yml
apiVersion: v1
kind: Service
metadata:
  name: mysql-service
spec:
  selector:
    app: mysql
  ports:
    - protocol: TCP
      port: 3306
      targetPort: 3306
#+end_src
Utilize o seguinte comando para aplicar a configuração do serviço MySQL.
#+begin_src sh
  kubectl apply -f mysql-serv.yml
#+end_src
** Aplicação do Deployment do Wordpress
Novamente, revise o seguinte arquivo de configuração do Deployment
para o Wordpress e modifique os campos que forem necessários, como o
para os arquivos específicos do Wordpress no servidor NFS.
#+begin_src txt :tangle wordpress/wordpress-dep.yml
# Arquivo: ./wordpress/wordpress-dep.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: wordpress-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: wordpress
  template:
    metadata:
      labels:
        app: wordpress
    spec:
      containers:
      - name: wordpress
        image: wordpress:latest
        env:
        - name: WORDPRESS_DB_HOST
          value: mysql-service
        - name: WORDPRESS_DB_USER
          value: root
        - name: WORDPRESS_DB_PASSWORD
          value: password
        - name: WORDPRESS_DB_NAME
          value: wordpress
        ports:
        - containerPort: 80
        volumeMounts:
        - name: nfs-volume
          mountPath: /var/www/html
      volumes:
      - name: nfs-volume
        nfs:
          server: 10.20.1.111
          path: /var/nfs/general/wordpress-igor
          readOnly: no
      tolerations:
      - effect: NoExecute
        key: node.kubernetes.io/not-ready
        operator: Exists
        tolerationSeconds: 30
      - effect: NoExecute
        key: node.kubernetes.io/unreachable
        operator: Exists
        tolerationSeconds: 30
#+end_src
Utilize o seguinte comando para aplicar o deployment do Wordpress.
#+begin_src sh
  kubectl apply -f wordpress-dep.yml
#+end_src
** Aplicação do Service do Wordpress
Revise o arquivo de configuração do serviço Wordpress e modifique os
campos que achar necessário.
#+begin_src txt :tangle wordpress/wordpress-serv.yml
# Arquivo: ./wordpress/wordpress-serv.yml
apiVersion: v1
kind: Service
metadata:
  name: wordpress-service
spec:
  selector:
    app: wordpress
  type: NodePort
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
      nodePort: 30036 
#+end_src
O seguinte comando aplica a configuração do serviço Wordpress.
#+begin_src sh
  kubectl apply -f wordpress-serv.yml
#+end_src
** Criação do banco de dados MySQL no container
Para que o Wordpress funcione precisamos criar, manualmente, um banco
de dados chamado 'wordpress'. Para isso, logamos no container do MySQL
e utilizamos o utilitário 'mysql' para emitir o comando de criação do
banco de dados.

A partir de control_plane logar no container:
#+begin_src sh
  kubectl get pods
  kubectl exec -it <MYSQL_POD_NAME> -- bash
  mysql -u root -p
  # no prompt do shell do mysql:
  create database wordpress;
  exit
  exit
#+end_src
* Instalação do Gitlab
** Aplicação do Deployment para o Gitlab
Revise o arquivo de configuração do deployment para o Gitlab e edite
os campos necessários, como os caminhos nos volumes "gitlab-data",
"gitlab-logs" e "gitlab-config" para servir a sua configuração. Vale
ressaltar que os caminhos para esses volumes devem ser diferentes.
#+begin_src txt :tangle gitlab/gitlab-dep.yml
# Arquivo: ./gitlab/gitlab-dep.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gitlab-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: gitlab
  template:
    metadata:
      labels:
        app: gitlab
    spec:
      containers:
      - name: gitlab
        image: gitlab/gitlab-ce:latest
        env:
        - name: GITLAB_OMNIBUS_CONFIG
          value: |
            external_url 'http://localhost' # Esse endereco deve ser o IP apontado pelo campo cluster-ip
        ports:
        - containerPort: 80
        volumeMounts:
        - name: gitlab-data
          mountPath: /var/opt/gitlab
        - name: gitlab-logs
          mountPath: /var/log/gitlab
        - name: gitlab-config
          mountPath: /etc/gitlab
      volumes:
      - name: gitlab-data
        nfs:
          server: 10.20.9.111
          path: /var/nfs/general/gitlab-igor/data
          readOnly: no
      - name: gitlab-logs
        nfs:
          server: 10.20.9.111
          path: /var/nfs/general/gitlab-igor/logs
          readOnly: no
      - name: gitlab-config
        nfs:
          server: 10.20.9.111
          path: /var/nfs/general/gitlab-igor/config
          readOnly: no
      tolerations:
      - effect: NoExecute
        key: node.kubernetes.io/not-ready
        operator: Exists
        tolerationSeconds: 30
      - effect: NoExecute
        key: node.kubernetes.io/unreachable
        operator: Exists
        tolerationSeconds: 30
#+end_src
Execute o seguinte comando para aplicar a configuração do deployment:
#+begin_src sh
  kubectl apply -f gitlab-dep.yml
#+end_src
** Aplicação do Service para o Gitlab
Revise o arquivo de configuração para o serviço do Gitlab e edite o
que achar necessário.
#+begin_src txt :tangle gitlab/gitlab-serv.yml
# Arquivo: ./gitlab/gitlab-serv
apiVersion: v1
kind: Service
metadata:
  name: gitlab-service
spec:
  selector:
    app: gitlab
  type: NodePort
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
      nodePort: 30036 
#+end_src
Execute o seguinte comando para aplicar a configuração:
#+begin_src sh
  kubectl apply -f gitlab-serv.yml
#+end_src
* Instalação de um Runner no Gitlab
Antes de de fato instalarmos o Runner do Gitlab, precisamos
registrá-lo no Gitlab. Para isso, instalamos o Runner num container Docker
apenas para registrá-lo no Gitlab. Depois descartamos o container e
instalamos outro Runner utilizando kubectl e utilizamos a configuração
do outro Runner, modificada, para configurar o novo Runner.
** Instalação do Docker
#+begin_src sh :tangle ./install-docker.sh :shebang "#!/bin/bash"
  # ./install-docker.sh
  dnf config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
  dnf upgrade --refresh -y
  dnf install docker-ce docker-ce-cli containerd.io
  systemctl start docker
#+end_src
** Logar numa instância Docker do Gitlab Runner
#+begin_src
docker run -it --entrypoint /bin/bash gitlab/gitlab-runner:latest
#+end_src
** Registrar o Gitlab Runner
*** Gerar um token para registrar o Gitlab Runner
Crie um repositório teste na sua instância do Gitlab. Acesse o
repositório teste e na tela do repositório vá em Settings -> CI/CD ->
Runners e siga as instruções para registrar um novo runner. O runner
no nosso caso deve utilizar a plataforma Linux e deve ser configurado
para executar trabalhados sem tag acionando o checkbox "Run untagged
jobs".

Você será direcionado para uma tela onde constará o token gerado.
*** Registrar
Volte para o shell logado no container Docker e execute:
#+begin_src sh
gitlab-runner register --url <CAMINHO_PARA_O_CLUSTER> --token <TOKEN_GERADO>
#+end_src
O CAMINHO_PARA_O_CLUSTER é o IP para qualquer nó do cluster (ex.:
http://10.20.9.116:30036).
Você pode visualizar o arquivo de configuração, gerado
automaticamente, do Runner com o comando:
#+begin_src sh
more /etc/gitlab-runner/config.toml
#+end_src
Nós vamos utilizar esse arquivo para configurar o nosso container
Kubernetes do Gitlab Runner. No meu caso o arquivo é estruturado
assim:
#+begin_src txt :tangle ./gitlab/config.toml
concurrent = 1
check_interval = 0
shutdown_timeout = 0

[session_server]
  session_timeout = 1800

[[runners]]
  name = "runner"
  url = "http://10.20.9.116:30036"
  id = 1
  token = "glrt-qJDS_pTGimZC8YtaoBPw"
  token_obtained_at = 2023-05-31T16:40:36Z
  token_expires_at = 0001-01-01T00:00:00Z
  executor = "docker"
  [runners.cache]
    MaxUploadedArchiveSize = 0
  [runners.docker]
    tls_verify = false
    image = "busybox:latest"
    privileged = false
    disable_entrypoint_overwrite = false
    oom_kill_disable = false
    disable_cache = false
    volumes = ["/cache"]
    shm_size =0 
#+end_src
Agora podemos deslogar do container Docker e derrubar o serviço Docker
e proceder para os próximos passos. Podemos também parar o serviço Docker:
#+begin_src sh
systemctl stop docker
#+end_src
*** Configurar o Gitlab Runner do Kubernetes
Para configurar o Gitlab Runner, precisamos adicionar um
ServiceAccount, um Role e um RoleBinding respectivo ao Runner no
cluster. O seguinte
arquivo provê essa configuração:
#+begin_src txt :tangle ./gitlab/gitlab-runner-authentication.yml
# Arquivo: ./gitlab/gitlab-runner-authentication.yml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: gitlab-admin
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: gitlab-admin
rules:
  - apiGroups: [""]
    resources: ["*"]
    verbs: ["*"]

---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: gitlab-admin
subjects:
  - kind: ServiceAccount
    name: gitlab-admin
roleRef:
  kind: Role
  name: gitlab-admin
  apiGroup: rbac.authorization.k8s.io
#+end_src
Aplique essa configuração com o comando:
#+begin_src sh
kubectl apply -f gitlab-runner-authentication.yml
#+end_src
Depois de termos um ServiceAccount, Role e RoleBinding configurados
precisamos de um ConfigMap para persistir a configuração do runner.
O arquivo de configuração do ConfigMap deve especificar o arquivo de
configuração do runner que roubamos do container Docker com algumas
modificações para adaptá-lo ao ambiente Kubernetes. No meu caso,
ficou assim:
#+begin_src txt :tangle ./gitlab/gitlab-runner-config.yml
# Arquivo: ./gitlab/gitlab-runner-config.yml
apiVersion: v1
kind: ConfigMap
metadata:
  name: gitlab-runner-config
data:
  config.toml: |-
     concurrent = 4
     [[runners]]
       name = "runner"
       url = "http://10.20.9.116:30036"
       id = 1
       token = "glrt-qJDS_pTGimZC8YtaoBPw"
       token_obtained_at = 2023-05-31T16:40:36Z
       token_expires_at = 0001-01-01T00:00:00Z
       executor = "kubernetes"
       [runners.kubernetes]
          poll_timeout = 600
          cpu_request = "1"
          service_cpu_request = "200m"
#+end_src
Após a aplicação dessa configuração, aplicamos a configuração do
Deployment:
#+begin_src txt :tangle ./gitlab/gitlab-runner-deployment.yml
# Arquivo: ./gitlab/gitlab-runner-deployment.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gitlab-runner
spec:
  replicas: 1
  selector:
    matchLabels:
      name: gitlab-runner
  template:
    metadata:
      labels:
        name: gitlab-runner
    spec:
      serviceAccountName: gitlab-admin
      containers:
        - args:
          - run
          image: gitlab/gitlab-runner:latest
          imagePullPolicy: Always
          name: gitlab-runner
          resources:
            requests:
              cpu: "100m"
            limits:
              cpu: "100m"
          volumeMounts:
            - name: config
              mountPath: /etc/gitlab-runner/config.toml
              readOnly: true
              subPath: config.toml
      volumes:
        - name: config
          configMap:
            name: gitlab-runner-config
      restartPolicy: Always
#+end_src
*** Verificar a instalação do Runner
Você pode verificar se a instalação do Runner obteve sucesso indo para
a tela Settings -> CI/CD -> Runners e checando se há um runner verde
na seção "Assigned project runners".
*** Referência
Vide [[https://adambcomer.com/blog/install-gitlab-runner-kubernetes/][referência]].
* Criação de pipeline de CI/CD no Gitlab
Pipelines de CI/CD (Continuous Integration / Continuous Delivery)
consistem em uma série de etapas a serem realizadas para a
disponibilização de uma nova versão de um software. Elas são uma
prática que tem como objetivo acelerar a disponibilização de
softwares, adotando a abordagem de DevOps ou de engenharia de
confiabilidade de sites (SRE). O pipeline de CI/CD inclui o
**monitoramento** e **automação** para melhorar o processo de
desenvolvimento de aplicações principalmente nos estágios de
integração e teste, mas também na entrega e na imnplantação.

Como solução de **monitoramente** utilizaremos o **Grafana**. Grafana é
uma plataforma de fonte aberta interativa de visualização de dados,
desenvolvieda pela Grafana Labs, que permite aos usuários ver dados
por meio de tabelas e gráficos unificados em um painel ou vários, para
facilitar a interpretação e a compreensão. Com o Grafana pode-se
consultar e definir alertas sobre informações e métricas de qualquer
lugar que os dados esteja, sejam ambientes tradicionais de servidor,
clusters do Kubernetes, etc. Grafana foi construído com base nos
princípios open source e na crença de que os dados devem ser
acessíveis em toda a organização, não apenas para um pequeno grupo de
pessoas. Isso promove uma cultura em que os dados podem ser
**facilmente encontrados e usados** por qualquer pessoa que precise
deles, capacitando as equipes a serem mais abertas, inovadoras e colaborativas.

**Prometheus** é um conjunto de ferramentas de fonte aberta para
monitoramento de sistemas. Ele coleta e armazena suas métricas como
dados de séries temporais, i.e., informações de métricas são
aramazenados com o /timestamp/ em que foi gravado.

Em nossa solução, utilizaremos o Prometheus para coletar as métricas
do Gitlab e o Grafana para tramar gráficos para visualização.
** Configuração do Gitlab
Configuração do Gitlab para permitir acesso do Prometheus.
** Instalação de Prometheus
*** Deploy
O seguinte arquivo cria o ClusterRole e ClusterRoleBinding para
configurar o mecanismo de [[https://kubernetes.io/docs/reference/access-authn-authz/rbac/][autorização]] para o Prometheus acessar
recursos protegidos do Kubernetes.

#+begin_src txt :tangle ./prometheus/alternative/kubernetes-prometheus/clusterRole.yaml
# Arquivo: ./prometheus/alternative/kubernetes-prometheus/clusterRole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole # contem regras que representam um conjunto de permissoes
metadata:
  name: prometheus
rules:
- apiGroups: [""] # "" indica o grupo de API base
  resources:      # especifica os recursos suplicados
  - nodes
  - nodes/proxy
  - services
  - endpoints
  - pods
  verbs: ["get", "list", "watch"]
- apiGroups:
  - extensions
  resources:
  - ingresses
  verbs: ["get", "list", "watch"]
- nonResourceURLs: ["/metrics"]
  verbs: ["get"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding # concede as permissoes definidas em um papel para um usuario ou conjunto de usuarios
metadata:
  name: prometheus
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus
subjects:
- kind: ServiceAccount
  name: default
  namespace: monitoring
#+end_src
#+begin_src txt :tangle ./prometheus/alternative/kubernetes-prometheus/config-map.yaml
# Arquivo: ./prometheus/alternative/kubernetes-prometheus/config-map.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-server-conf
  labels:
    name: prometheus-server-conf
  namespace: monitoring
data:
  prometheus.rules: |-
    groups:
    - name: devopscube demo alert
      rules:
      - alert: High Pod Memory
        expr: sum(container_memory_usage_bytes) > 1
        for: 1m
        labels:
          severity: slack
        annotations:
          summary: High Memory Usage
  prometheus.yml: |-
    global:
      scrape_interval: 5s
      evaluation_interval: 5s
    rule_files:
      - /etc/prometheus/prometheus.rules
    alerting:
      alertmanagers:
      - scheme: http
        static_configs:
        - targets:
          - "alertmanager.monitoring.svc:9093"
    scrape_configs:
      - job_name: 'node-exporter'
        kubernetes_sd_configs:
          - role: endpoints
        relabel_configs:
        - source_labels: [__meta_kubernetes_endpoints_name]
          regex: 'node-exporter'
          action: keep
      - job_name: 'kubernetes-apiservers'
        kubernetes_sd_configs:
        - role: endpoints
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
        - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
          action: keep
          regex: default;kubernetes;https
      - job_name: 'kubernetes-nodes'
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        kubernetes_sd_configs:
        - role: node
        relabel_configs:
        - action: labelmap
          regex: __meta_kubernetes_node_label_(.+)
        - target_label: __address__
          replacement: kubernetes.default.svc:443
        - source_labels: [__meta_kubernetes_node_name]
          regex: (.+)
          target_label: __metrics_path__
          replacement: /api/v1/nodes/${1}/proxy/metrics
      - job_name: 'kubernetes-pods'
        kubernetes_sd_configs:
        - role: pod
        relabel_configs:
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
          action: keep
          regex: true
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
          action: replace
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
          target_label: __address__
        - action: labelmap
          regex: __meta_kubernetes_pod_label_(.+)
        - source_labels: [__meta_kubernetes_namespace]
          action: replace
          target_label: kubernetes_namespace
        - source_labels: [__meta_kubernetes_pod_name]
          action: replace
          target_label: kubernetes_pod_name
      - job_name: 'kubernetes-cadvisor'
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        kubernetes_sd_configs:
        - role: node
        relabel_configs:
        - action: labelmap
          regex: __meta_kubernetes_node_label_(.+)
        - target_label: __address__
          replacement: kubernetes.default.svc:443
        - source_labels: [__meta_kubernetes_node_name]
          regex: (.+)
          target_label: __metrics_path__
          replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor
      - job_name: 'kubernetes-service-endpoints'
        kubernetes_sd_configs:
        - role: endpoints
        relabel_configs:
        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
          action: keep
          regex: true
        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
          action: replace
          target_label: __scheme__
          regex: (https?)
        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
          action: replace
          target_label: __address__
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
        - action: labelmap
          regex: __meta_kubernetes_service_label_(.+)
        - source_labels: [__meta_kubernetes_namespace]
          action: replace
          target_label: kubernetes_namespace
        - source_labels: [__meta_kubernetes_service_name]
          action: replace
          target_label: kubernetes_name
      - job_name: redis
        static_configs:
          - targets:
            - 10.20.9.116:30091
      - job_name: postgres
        static_configs:
          - targets:
            - 10.20.9.116:30092
      - job_name: node
        static_configs:
          - targets:
            - 10.20.9.116:30086
      - job_name: gitlab-workhorse
        static_configs:
          - targets:
            - 10.20.9.116:30087
      - job_name: gitlab-rails
        metrics_path: "/-/metrics"
        scheme: http
        static_configs:
          - targets:
            - 10.20.9.116:30036
      - job_name: gitlab-sidekiq
        static_configs:
          - targets:
            - 10.20.9.116:30090
      - job_name: gitlab_exporter_database
        metrics_path: "/database"
        static_configs:
          - targets:
            - 10.20.9.116:30088
      - job_name: gitaly
        static_configs:
          - targets:
            - 10.20.9.116:30093
#+end_src
#+begin_src txt :tangle ./prometheus/alternative/kubernetes-prometheus/prometheus-deployment.yaml
# Arquivo: ./prometheus/alternative/kubernetes-prometheus/prometheus-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus-deployment
  namespace: monitoring
  labels:
    app: prometheus-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus-server
  template:
    metadata:
      labels:
        app: prometheus-server
    spec:
      containers:
        - name: prometheus
          image: prom/prometheus
          args:
            - "--config.file=/etc/prometheus/prometheus.yml"
            - "--storage.tsdb.path=/prometheus/"
          ports:
            - containerPort: 9090
          volumeMounts:
            - name: prometheus-config-volume
              mountPath: /etc/prometheus/
            - name: prometheus-storage-volume
              mountPath: /prometheus/
      volumes:
        - name: prometheus-config-volume
          configMap:
            defaultMode: 420
            name: prometheus-server-conf
  
        - name: prometheus-storage-volume
          emptyDir: {}
#+end_src
#+begin_src txt :tangle ./prometheus/alternative/kubernetes-prometheus/prometheus-service.yaml
# Arquivo: ./prometheus/alternative/kubernetes-prometheus/prometheus-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: prometheus-service
  namespace: monitoring
  annotations:
      prometheus.io/scrape: 'true'
      prometheus.io/port:   '9090'
  
spec:
  selector: 
    app: prometheus-server
  type: NodePort  
  ports:pp
    - port: 8080
      targetPort: 9090 
      nodePort: 30000
#+end_src
*** Criação de services para expor endpoints dentro do container do Gitlab
Esses serviçoes poderiam ser descritos no arquvivo
./gitlab/gitlab-serv.yaml adicionando múltiplas portas ([[https://kubernetes.io/docs/concepts/services-networking/service/#multi-port-services][Vide]]).

Nota: todos os serviços de métrica são atingíveis exceto pelo
node-exporter e gitlab-rails. O gitlab-rails precisa de configuração
pelo dashboard e o node-exporter provavelmente não é levantado com a
configuração minimalista entregue pela documentação do Gitlab.

Att: o node-explorer pode ser habilitado descomentando
"node_explorer['enable'] = true".

Configuração para expor serviços internos do Gitlab:
#+begin_src txt :tangle ./gitlab/services-integration-prometheus/integration-service.yaml
# Arquivo: ./gitlab/services-integration-prometheus/integration-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: integration-service
spec:
  selector:
    app: gitlab
  type: NodePort
  ports:
    - name: node-exporter-metrics
      protocol: TCP
      port: 9100
      targetPort: 9100
      nodePort: 30086
    - name: gitlab-workhorse-metrics
      protocol: TCP
      port: 9229
      targetPort: 9229
      nodePort: 30087
    - name: gitlab-exporter-metrics
      protocol: TCP
      port: 9168
      targetPort: 9168
      nodePort: 30088
    - name: registry-metrics
      protocol: TCP
      port: 5001
      targetPort: 5001
      nodePort: 30089
    - name: sidekiq-metrics
      protocol: TCP
      port: 8082
      targetPort: 8082
      nodePort: 30090
    - name: redis-exporter-metrics
      protocol: TCP
      port: 9121
      targetPort: 9121
      nodePort: 30091
    - name: postgres-metrics
      protocol: TCP
      port: 9187
      targetPort: 9187
      nodePort: 30092
    - name: gitaly-metrics
      protocol: TCP
      port: 9236
      targetPort: 9236
      nodePort: 30093
    - name: pgbouncer-metrics
      protocol: TCP
      port: 9188
      targetPort: 9188
      nodePort: 30094
#+end_src
** Instalação de Grafana
*** Deployment
#+begin_src txt :tangle ./grafana/grafana.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: grafana
  name: grafana
spec:
  selector:
    matchLabels:
      app: grafana
  template:
    metadata:
      labels:
        app: grafana
    spec:
      securityContext:
        fsGroup: 472
        supplementalGroups:
          - 0
      containers:
        - name: grafana
          image: grafana/grafana:9.1.0
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 3000
              name: http-grafana
              protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /robots.txt
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 2
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: 3000
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 250m
              memory: 750Mi
          volumeMounts:
            - mountPath: /var/lib/grafana
              name: grafana-data
      volumes:
        - name: grafana-data
          nfs:
            server: 10.20.9.111
            path: /var/nfs/general/grafana-igor
            readOnly: no
---
apiVersion: v1
kind: Service
metadata:
  name: grafana
spec:
  ports:
    - port: 3000
      protocol: TCP
      targetPort: http-grafana
      nodePort: 30001
  selector:
    app: grafana
  sessionAffinity: None
  type: NodePort
#+end_src
#+begin_src sh
  kubectl apply -f grafana.yaml
#+end_src
Para fazer login na sua instância do Grafana, utilize como credênciais
'admin' para usuário e 'admin' para senha.
* Configurações alternativas
** Deploy do Prometheus
Para deploy do Prometheus pode-se utilizar a seguinte
configuração. Entretanto, ela é complexa e difícil de configurar.
*** Deploy via Helm Charts
#+begin_src sh
  kubectl create namespace darwin
  helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
  helm install prometheus prometheus-community/kube-prometheus-stack --namespace darwin --version 46.8.0
#+end_src
#+begin_src txt :tangle ./prometheus/prometheus-config.yaml
# Arquivo: ./prometheus/prometheus-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: darwin
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      scrape_timeout: 10s
      evaluation_interval: 15s
    scrape_configs:
      - job_name: 'darwin-service'
        scrape_interval: 5s
        static_configs:
          - targets: ['darwin-service:8080']
      - job_name: nginx
        static_configs:
          - targets:
            - 1.1.1.1:8060
      - job_name: redis
        static_configs:
          - targets:
            - 1.1.1.1:9121
      - job_name: postgres
        static_configs:
          - targets:
            - 1.1.1.1:9187
      - job_name: node
        static_configs:
          - targets:
            - 1.1.1.1:9100
      - job_name: gitlab-workhorse
        static_configs:
          - targets:
            - 1.1.1.1:9229
      - job_name: gitlab-rails
        metrics_path: "/-/metrics"
        scheme: https
        static_configs:
          - targets:
            - 1.1.1.1
      - job_name: gitlab-sidekiq
        static_configs:
          - targets:
            - 1.1.1.1:8082
      - job_name: gitlab_exporter_database
        metrics_path: "/database"
        static_configs:
          - targets:
            - 1.1.1.1:9168
      - job_name: gitlab_exporter_sidekiq
        metrics_path: "/sidekiq"
        static_configs:
          - targets:
            - 1.1.1.1:9168
      - job_name: gitaly
        static_configs:
          - targets:
            - 1.1.1.1:9236
#+end_src
#+begin_src txt :tangle ./prometheus/darwin-prometheus-service.yaml
# Arquivo: ./prometheus/darwin-prometheus-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: darwin-prometheus-service
  namespace: darwin
spec:
  type: NodePort
  selector:
    app.kubernetes.io/name: prometheus
  ports:
    - name: web
      port: 9090
      targetPort: 9090
      nodePort: 30000
#+end_src

