#+TITLE: Roteiro Kubernetes

* Introdução
Essa documentação utiliza princípios de programação
letrada. Consequentemente, alguns snippets que são descritos abaixo então
contidos em scripts extraídos da documentação. O nome do script
que contém o snippet respectivo está em um comentário na primeira
linha do snippet. 

Todos os scripts e comandos devem
ser executados como usuário raiz.

O login de usuário root via SSH deve
estar habilitado também.
* Clonar VMs
A clonagem das VMs é feita através do ProxMox manualmente.
* SSH
Criação de chaves assimétricas para acesso SSH sem senha. Esse passo deve ser
feito manualmente.
** Cria chaves assimétricas
#+begin_src sh
  # A variável HOSTS deve conter os IPs dos hosts da instalação do cluster.
  # Exemplo: HOSTS=(10.20.1.123 10.20.1.124)
  HOSTS=()
  ssh-keygen -q -t rsa -b 4096 -f ~/.ssh/id_rsa -P ""
  for HOST in "${HOSTS[@]}"; do ssh-copy-id $USER@$HOST; done
#+end_src
* Instalar Python
Instalação de Python e Pip no host de onde partirá a instalação
do kubernetes. O seguinte snippet instala Python e Pip em sistemas
baseados em RPM. Caso o sistema ponto de partida não seja baseado em
RPM, deve-se utilizar os comandos cabíveis.
#+begin_src sh :tangle deps-install.sh
  # ./deps-install.sh
  sudo dnf install python39 python3-pip -y
  pip3 install --upgrade pip
#+end_src
* Clonar repositório do Kubespray e instalar dependências.
Caso a instalação esteja partindo de uma máquina exterior ao cluster,
pode-se utilizar o seguinte script; mas se a instalação estiver
partindo de um dos nós do cluster, deve-se omitir a instalação do do
ambiente virtual Python.

O script abaixo instala as dependências Python do Kubespray no
diretório /tmp/repos/kubespray-venv. Segundo a [[https://docs.python.org/3/library/venv.html][documentação]] do Python,
esses ambientes virtuais servem para resolver problemas de
incompatibilidade entre versões de um mesmo software requerido em
várias versões. Em alguns dos scripts/snippets que seguem,
utilizaremos essas dependências por conveniência.
#+begin_src sh :tangle kubespray-install.sh
	# ./kubespray-install.sh
	if [ ! -d /tmp/repos ]; then
		mkdir /tmp/repos
	fi

	cd /tmp/repos

	if [ ! -d ./kubespray ]; then
		git clone https://github.com/kubernetes-sigs/kubespray
	fi

	EXTERNAL=""
	read -p "A instalação do cluster parte de uma máquina externa ao cluster? (y/N)" EXTERNAL
	if [ $EXTERNAL = "y" ]; then
	  pip install virtualenv

	  VENVDIR=kubespray-venv
	  KUBESPRAYDIR=kubespray
	  ANSIBLE_VERSION=2.12
	  virtualenv  --python=$(which python3) $VENVDIR
	  source $VENVDIR/bin/activate
	  cd $KUBESPRAYDIR
	  pip install -U -r requirements-$ANSIBLE_VERSION.txt
	elif [ $EXTERNAL ="N" ]; then 
	  cd kubespray
	  pip install -U -r requirements.txt
	else
	  echo "Entrada inválida"
	fi
  #+end_src
* Atualizar sistemas
Caso não tenha sido feito ou precisa fazer novamente, atualizar os
sistemas operacionais. Esse script depende da existência dos arquivos
"inventory.ini" e "update-systems-playbook.yaml" que são providos
nesse repositório.
#+begin_src sh :tangle update-systems.sh
  # ./update-systems.sh
  (
	  source /tmp/repos/kubespray-venv/bin/activate
	  ansible-playbook -i inventory.ini update-systems-playbook.yaml --become --become-user=root
  )
#+end_src
Playbook Ansible que especifica a tarefa para atualizar os pacotes do
sistema operacional alvo para as versões mais recentes da distribuição.
#+begin_src yml :tangle update-systems-playbook.yaml
- name: Atualiza sistema
  hosts: server*
  become: true
  tasks:
  - name: Atualiza sistema
    package:
      name: '*'
      state: latest
#+end_src
Inventário contendo configuração para acesso aos hosts. Esse arquivo
deve ser atualizado com os IPs dos hosts do cluster manualmente.
#+begin_src text :tangle inventory.ini
server1 ansible_host=10.20.1.113
server2 ansible_host=10.20.1.115
#+end_src
* Desabilitar firewall
Caso os firewalls não tenham sido desabilitados, desabilitá-los.
#+begin_src sh :tangle remove-firewall.sh
  # ./remove-firewall.sh
  (
	  source /tmp/repos/kubespray-venv/bin/activate
	  ansible-playbook -i inventory.ini remove-firewall-playbook.yaml --become --become-user=root
  )
#+end_src
#+begin_src yml :tangle remove-firewall-playbook.yaml
- name: Remove firewall e habilitar login de root via SSH
  hosts: server*
  become: true
  tasks:
  - name: Remove firewall
    shell: |
      systemctl stop firewalld.service
      systemctl disable firewalld.service
#+end_src
* Próximos passos
Próximos passos são documentados no repositório oficial do [[https://github.com/kubernetes-sigs/kubespray][Kubespray]].
Mas há algumas ressalvas. A instalação utilizando ambientes virtuais
do python quando a instalação é feita a partir de uma máquina que será
um nó do cluster apresenta erros na busca de dependências do python.
Caso a máquina de partida da instalação deva ser um nó esse passo pode
ser omitido de qualquer modo.

Os seguintes passos, que são os mais importantes, devem ser executados manualmente:
#+begin_src sh
  # Na raíz do projeto kubespray.
  cp -rfp inventory/sample inventory/mycluster
  # IPS é um vetor contendo os IPs dos hosts do cluster.
  declare -a IPS=()

  # A seguinte linha deve ser executada somente se a máquina de onde parte
  # a instalação for externa ao cluster.
  source ../kubespray-venv/bin/activate

  # Esse script gera o inventário automaticamente com configuração padrão.
  CONFIG_FILE=inventory/mycluster/hosts.yaml python3 contrib/inventory_builder/inventory.py ${IPS[@]}
  # Nesse ponto pode-se revisar e modificar as variáveis em
  # inventory/mycluster/group_vars/all/all.yml e
  # inventory/mycluster/group_vars/k8s_cluster/k8s_cluster.yml.

  # Para limpar um cluster velho, executar como root:
  ansible-playbook -i inventory/mycluster/hosts.yaml --become --become-user=root reset.yml
  # Para fazer uma nova instalação do kubernetes, executar como root:
  ansible-playbook -i inventory/mycluster/hosts.yaml --become --become-user=root cluster.yml
#+end_src
* Instalação do Wordpress + MySQL
Os passos utilizados para instalação do Wordpress e MySQL consistem
na aplicação de um [[https://kubernetes.io/docs/concepts/workloads/controllers/deployment/][deployment]] e de um [[https://kubernetes.io/docs/concepts/services-networking/service/][service]] para cada
componente. Cada deployment possuirá também uma configuração para
utilizar um servidor NFS como armazenamento persistente, com o intúito
de preservar a configuração das aplicações e e dos dados do banco de dados entre
possíveis deployments (ex.: um nó é desligo ou cai).
Também possuirá uma configuração que especifica uma [[https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/][toleration]], i.e.,
um espaço de tempo que um container permanecerá atrelado a um nó
enquanto uma taint for verificada, por exemplo, quando a taint
not-ready estiver verificada quando o nó estiver fora do ar.

** Instalação do servidor NFS
Antes de tudo, executamos o seguinte roteiro para criar um servidor
NFS. No host onde o servidor NFS será hospedado, executar, como raiz:
#+begin_src sh
  # Como raiz.
  dnf install nfs-utils -y
  mkdir /var/nfs/general -p
  touch /etc/exports
  
  # Colocar IPs dos workers no vetor HOSTS.
  # Exemplo: HOSTS=(10.20.1.113 10.20.1.118)
  export HOSTS=()
  # Fazemos export para podermos utilizar essa variável em outros momentos, caso cabível.
  for i in "${HOSTS[@]}"; do echo "/var/nfs/general $i(rw,no_subtree_check,no_root_squash)" >> /etc/exports;done

  systemctl enable nfs-server
  systemctl start nfs-server

  # Esse comando deve ser executado toda vez que o arquivo /etc/exports
  # for modificado.
  exportfs -ra

  # Os hosts clientes também precisam do pacote nfs-utils, caso não estejam instalados
  # então instalamos ele:
  for i in "${HOSTS}"; do ssh $USER@$i "dnf install nfs-utils -y";done
#+end_src
Caso o servidor NFS já exista, deve-se executar apenas os seguintes comandos no servidor:
#+begin_src sh
  # Modificar manualemente o arquivo /etc/exports
  # ou então executar o seguinte snippet.
  HOSTS=()
  for i in "${HOSTS[@]}"; do echo "/var/nfs/general $i(rw,no_subtree_check,no_root_squash)" >> /etc/exports;done

  exportfs -ra
#+end_src
Os comandos acima especificam o diretório a ser montado nos clientes,
os IPS dos clientes e configurações por IP.

Deve-se também criar pastas específicas de cada aplicação no diretório
/var/nfs/general/
(ex.: /var/nfs/general/mysql-igor)
e deixá-las com permissão 777 para evitar erros de permissão e também
com usuário e grupo nobody.
#+begin_src sh
  chmod 777 -R /var/nfs/general/<DIR>
  chown nobody:nobody -R /var/nfs/general/<DIR>
#+end_src
** Aplicação do Deployment do MySQL
Em um dos master nodes (control_planes) executar os seguintes comandos:
#+begin_src sh
  kubectl apply -f mysql-dep.yml
#+end_src
#+begin_src txt :tangle mysql-dep.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mysql-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql:latest
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: password
        ports:
        - containerPort: 3306
        volumeMounts:
        - name: nfs-volume
          mountPath: /var/lib/mysql
      volumes:
      - name: nfs-volume
        nfs:
          server: 10.20.1.111
          path: /var/nfs/general/mysql-igor
          readOnly: no
      tolerations:
      - effect: NoExecute
        key: node.kubernetes.io/not-ready
        operator: Exists
        tolerationSeconds: 30
      - effect: NoExecute
        key: node.kubernetes.io/unreachable
        operator: Exists
        tolerationSeconds: 30
#+end_src
O seguintes comandos podem ser utilizados para resgatar informações básicas
Sobre o deployment e sobre o pod criado.
#+begin_src sh
  kubectl get deployment -o wide
  kubectl get pod -o wide
#+end_src
** Aplicação do Service do MySQL
#+begin_src sh
  kubectl apply -f mysql-serv.yml
#+end_src
#+begin_src txt :tangle mysql-serv.yml
apiVersion: v1
kind: Service
metadata:
  name: mysql-service
spec:
  selector:
    app: mysql
  ports:
    - protocol: TCP
      port: 3306
      targetPort: 3306
#+end_src
O seguintes comandos podem ser utilizados para resgatar informações básicas
sobre o Service criado.
#+begin_src sh
  kubectl get svc -o wide
#+end_src
** Aplicação do Deployment do Wordpress
#+begin_src sh
  kubectl apply -f wordpress-dep.yml
#+end_src
#+begin_src txt :tangle wordpress-dep.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: wordpress-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: wordpress
  template:
    metadata:
      labels:
        app: wordpress
    spec:
      containers:
      - name: wordpress
        image: wordpress:latest
        env:
        - name: WORDPRESS_DB_HOST
          value: mysql-service
        - name: WORDPRESS_DB_USER
          value: root
        - name: WORDPRESS_DB_PASSWORD
          value: password
        - name: WORDPRESS_DB_NAME
          value: wordpress
        ports:
        - containerPort: 80
        volumeMounts:
        - name: nfs-volume
          mountPath: /var/www/html
      volumes:
      - name: nfs-volume
        nfs:
          server: 10.20.1.111
          path: /var/nfs/general/wordpress-igor
          readOnly: no
      tolerations:
      - effect: NoExecute
        key: node.kubernetes.io/not-ready
        operator: Exists
        tolerationSeconds: 30
      - effect: NoExecute
        key: node.kubernetes.io/unreachable
        operator: Exists
        tolerationSeconds: 30
#+end_src
** Aplicação do Service do Wordpress
#+begin_src sh
  kubectl apply -f wordpress-serv.yml
#+end_src
#+begin_src txt :tangle wordpress-serv.yml
apiVersion: v1
kind: Service
metadata:
  name: wordpress-service
spec:
  selector:
    app: wordpress
  type: NodePort
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
      nodePort: 30036 
#+end_src
Os seguintes comandos podem ser utilizados para fazer troubleshooting
nos respectivos componentes Kubernetes:
#+begin_src sh
  kubectl describe deployment
  kubectl describe pod
  kubectl describe service
  kubectl logs <POD_NAME>
#+end_src
** Criação do banco de dados MySQL no container
A partir de control_plane logar no container:
#+begin_src sh
  kubectl exec -it <MYSQL_POD_NAME> -- bash
  mysql -u root -p
  # no prompt do shell do mysql:
  create database wordpress;
  exit
  exit
#+end_src
* Instalação do Gitlabc
* Problemas encontrados
** 04-24-2023, 14:12
-  Após a instalação com sucesso houve algumas falhas: 1 nó
   configurado como control_plane não consta como control_plano na
   saída do comando "kubectl get node".
-  Após a instalação 2 nós constam como status "NotReady".
-  Os passos para chegar nessa situação foram:
   + Primeiro foi feita uma tentativa de instalação com inventário
     manualmente escrito. Nessa tentativa houve 1 único erro em todos
     os nós que dizia respeito ao serviço de firewall do SO.
   + Depois foi feita uma tentativa com um inventário escrito
     automaticamente pelo script como está na documentação do
     Kubespray. O script não modificou coisas relevantes no
     inventário. Nessa instalação houveram erros que diziam respeito
     ao nome de módulos de kernel, mas o sumário ao fim da instalação
     não constavam como se a instalação tivesse sido prejudicada por isso.
** 04-25-2023, 12:00
- Instalação do Kubernetes utilizando ambientes virtuais do python
  apresenta erros quando a máquina de onde parte a instalação será um
  nó do cluster. Com a utilização dos ambientes virtuais do python, o
  interpretador python utilizado é o que está dentro do ambiente
  virtual. O ansible não consegue encontrar o módulo selinux-python,
  nesse caso, mas consegue encontrar caso utilizemos o ambiente global
  do python.

